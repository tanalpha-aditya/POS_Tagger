{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preparing the Data","metadata":{}},{"cell_type":"code","source":"# import resources\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:43.717584Z","iopub.execute_input":"2023-03-18T12:42:43.717985Z","iopub.status.idle":"2023-03-18T12:42:45.939794Z","shell.execute_reply.started":"2023-03-18T12:42:43.717950Z","shell.execute_reply":"2023-03-18T12:42:45.938693Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def read_conllu_file(filename):\n    index = []\n    words = []\n    pos_tags = []\n    word_sentence = []\n    pos_tags_sentences = []\n    index_sentence = []\n    words_total = []\n    pos_tags_total = []\n\n    count = 0\n    with open(filename, \"r\") as f:\n        lines = f.readlines()\n        for line in lines:\n            if line.strip():\n                if line.startswith(\"#\"):\n                    continue\n                fields = line.strip().split(\"\\t\")\n                if \".\" in fields[0]:\n                    continue\n                index_sentence.append(fields[0])\n                word_sentence.append(fields[1])\n                pos_tags_sentences.append(fields[3])\n                words_total.append(fields[1])\n                pos_tags_total.append(fields[3])\n            else:\n                index.append(index_sentence)\n                print\n                pos_tags.append(pos_tags_sentences)\n                words.append(word_sentence)\n                word_sentence = []\n                pos_tags_sentences = []\n                index_sentence = []\n#             print ( fields[0])\n    return index, words, pos_tags, words_total, pos_tags_total","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:45.941589Z","iopub.execute_input":"2023-03-18T12:42:45.942024Z","iopub.status.idle":"2023-03-18T12:42:45.952420Z","shell.execute_reply.started":"2023-03-18T12:42:45.941998Z","shell.execute_reply":"2023-03-18T12:42:45.949693Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"index, words, pos_tags, words_total, pos_tags_total= read_conllu_file(\"/kaggle/input/datasent-for-pos-tagging-iiith/en_atis-ud-train.conllu\")\nindex_dev, words_dev, pos_tags_dev, words_total_dev, pos_tags_total_dev= read_conllu_file(\"/kaggle/input/datasent-for-pos-tagging-iiith/en_atis-ud-dev.conllu\")\nindex_test, words_test, pos_tags_test, words_total_test, pos_tags_total_test= read_conllu_file(\"/kaggle/input/datasent-for-pos-tagging-iiith/en_atis-ud-test.conllu\")\n","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:45.953739Z","iopub.execute_input":"2023-03-18T12:42:45.954722Z","iopub.status.idle":"2023-03-18T12:42:46.279236Z","shell.execute_reply.started":"2023-03-18T12:42:45.954353Z","shell.execute_reply":"2023-03-18T12:42:46.278336Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(words_dev[0])","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:46.280301Z","iopub.execute_input":"2023-03-18T12:42:46.281805Z","iopub.status.idle":"2023-03-18T12:42:46.289329Z","shell.execute_reply.started":"2023-03-18T12:42:46.281772Z","shell.execute_reply":"2023-03-18T12:42:46.288211Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"['i', 'would', 'like', 'the', 'cheapest', 'flight', 'from', 'pittsburgh', 'to', 'atlanta', 'leaving', 'april', 'twenty', 'fifth', 'and', 'returning', 'may', 'sixth']\n","output_type":"stream"}]},{"cell_type":"code","source":"num_sentences = len(index)\nnum_words = len(set(words_total))\nnum_tags = len(set(pos_tags_total))\nprint( \" Total number of sentences \", num_sentences)\nprint( \" Total number of unique words \", num_words)\nprint( \" Total number of tags \", num_tags)\n# print( set ( pos_tags))","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:46.292417Z","iopub.execute_input":"2023-03-18T12:42:46.292903Z","iopub.status.idle":"2023-03-18T12:42:46.319024Z","shell.execute_reply.started":"2023-03-18T12:42:46.292872Z","shell.execute_reply":"2023-03-18T12:42:46.317844Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":" Total number of sentences  4274\n Total number of unique words  863\n Total number of tags  13\n","output_type":"stream"}]},{"cell_type":"code","source":"num_sentences_dev = len(index_dev)\nnum_words_dev = len(set(words_total_dev))\nnum_tags_dev = len(set(pos_tags_total_dev))\nprint( \" Total number of sentences \", num_sentences_dev)\nprint( \" Total number of unique words \", num_words_dev)\nprint( \" Total number of tags \", num_tags_dev)\nprint(set(pos_tags_total_dev))","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:46.320498Z","iopub.execute_input":"2023-03-18T12:42:46.321546Z","iopub.status.idle":"2023-03-18T12:42:46.332276Z","shell.execute_reply.started":"2023-03-18T12:42:46.321513Z","shell.execute_reply":"2023-03-18T12:42:46.331255Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":" Total number of sentences  570\n Total number of unique words  448\n Total number of tags  13\n{'ADV', 'PART', 'ADJ', 'INTJ', 'NOUN', 'ADP', 'VERB', 'AUX', 'PROPN', 'NUM', 'CCONJ', 'PRON', 'DET'}\n","output_type":"stream"}]},{"cell_type":"code","source":"num_sentences_test = len(index_test)\nnum_words_test = len(set(words_total_test))\nnum_tags_test = len(set(pos_tags_total_test))\nprint( \" Total number of sentences \", num_sentences_test)\nprint( \" Total number of unique words \", num_words_test)\nprint( \" Total number of tags \", num_tags_test)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:46.334137Z","iopub.execute_input":"2023-03-18T12:42:46.335389Z","iopub.status.idle":"2023-03-18T12:42:46.345468Z","shell.execute_reply.started":"2023-03-18T12:42:46.335347Z","shell.execute_reply":"2023-03-18T12:42:46.344664Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":" Total number of sentences  586\n Total number of unique words  476\n Total number of tags  13\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Encoding and Embedding","metadata":{}},{"cell_type":"code","source":"import torchtext\nfrom torchtext.vocab import Vocab\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:46.346896Z","iopub.execute_input":"2023-03-18T12:42:46.347988Z","iopub.status.idle":"2023-03-18T12:42:46.462019Z","shell.execute_reply.started":"2023-03-18T12:42:46.347957Z","shell.execute_reply":"2023-03-18T12:42:46.461063Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"word_to_idx = {}\nidx_to_word = {}\n\nunique_words = set(words_total)\n\n# print( unique_words)\nfor i, word in enumerate(unique_words):\n    word_to_idx[word] = i\n    idx_to_word[i] = word\n    \nX = []\nfor sent in words:\n#     print(sent)\n    encoded_sent = [word_to_idx[word] for word in sent]\n    X.append(encoded_sent)\n    \nX_test = []\nfor sent in words_test:\n    encoded_sent = []\n    for word in sent:\n        if word in unique_words:\n            encoded_sent.append(word_to_idx[word])\n        else:\n            encoded_sent.append(0)\n    X_test.append(encoded_sent)\n\ncount = 0 \ncounta = 0 \nX_dev = []\nfor sent in words_dev:\n    encoded_sent = []\n    for word in sent:\n        if word in unique_words:\n            encoded_sent.append(word_to_idx[word])\n            counta +=1\n        else:\n            encoded_sent.append(0)\n            count+=1\n    X_dev.append(encoded_sent)\n    \n# print(X_dev[4])\nprint(count, counta)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:46.463182Z","iopub.execute_input":"2023-03-18T12:42:46.463611Z","iopub.status.idle":"2023-03-18T12:42:46.491461Z","shell.execute_reply.started":"2023-03-18T12:42:46.463584Z","shell.execute_reply":"2023-03-18T12:42:46.489901Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"32 6598\n","output_type":"stream"}]},{"cell_type":"code","source":"word_to_idx_tag = {}\nidx_to_word_tag = {}\n\nunique_words = set(pos_tags_total)\n# print( unique_words)\nfor i, word in enumerate(unique_words):\n    word_to_idx_tag[word] = i\n    idx_to_word_tag[i] = word\n    \nY = []\nfor sent in pos_tags:\n    encoded_sent = [word_to_idx_tag[word] for word in sent]\n    Y.append(encoded_sent)\n    \nY_dev = []\nfor sent in pos_tags_dev:\n    encoded_sent = [word_to_idx_tag[word] for word in sent]\n    Y_dev.append(encoded_sent)\n\nY_test = []\nfor sent in pos_tags_test:\n    encoded_sent = [word_to_idx_tag[word] for word in sent]\n    Y_test.append(encoded_sent)\n\nprint(Y_dev[0])","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:46.493060Z","iopub.execute_input":"2023-03-18T12:42:46.493362Z","iopub.status.idle":"2023-03-18T12:42:46.512797Z","shell.execute_reply.started":"2023-03-18T12:42:46.493333Z","shell.execute_reply":"2023-03-18T12:42:46.511769Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[11, 6, 8, 12, 2, 5, 4, 7, 4, 7, 8, 5, 9, 2, 10, 8, 5, 2]\n","output_type":"stream"}]},{"cell_type":"code","source":"# word_to_idx = {}\n# idx_to_word = {}\n\n# unique_words_dev = set(pos_tags_total_dev)\n# # print( unique_words)\n# for i, word in enumerate(unique_words_dev):\n#     word_to_idx[word] = i\n#     idx_to_word[i] = word\n# Y_dev = []\n# for sent in pos_tags_dev:\n#     encoded_sent = [word_to_idx[word] for word in sent]\n#     Y_dev.append(encoded_sent)    \n\n    \n# print(Y_dev[0])","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:46.514311Z","iopub.execute_input":"2023-03-18T12:42:46.514836Z","iopub.status.idle":"2023-03-18T12:42:46.531896Z","shell.execute_reply.started":"2023-03-18T12:42:46.514809Z","shell.execute_reply":"2023-03-18T12:42:46.530758Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# word_to_idx = {}\n# idx_to_word = {}\n\n# unique_words_test = set(pos_tags_total_test)\n# # print( unique_words)\n# for i, word in enumerate(unique_words_test):\n#     word_to_idx[word] = i\n#     idx_to_word[i] = word\n    \n\n    \n# print(Y_test[0])","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:46.533117Z","iopub.execute_input":"2023-03-18T12:42:46.533386Z","iopub.status.idle":"2023-03-18T12:42:46.547676Z","shell.execute_reply.started":"2023-03-18T12:42:46.533354Z","shell.execute_reply":"2023-03-18T12:42:46.546848Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print( idx_to_word)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:46.549328Z","iopub.execute_input":"2023-03-18T12:42:46.549597Z","iopub.status.idle":"2023-03-18T12:42:46.563974Z","shell.execute_reply.started":"2023-03-18T12:42:46.549570Z","shell.execute_reply":"2023-03-18T12:42:46.562561Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"{0: '1994', 1: 'november', 2: 'during', 3: 'either', 4: 'dallas', 5: 'combination', 6: 'four', 7: '1200', 8: '1026', 9: 'an', 10: 'serve', 11: 'trip', 12: 'lester', 13: 'buy', 14: 'arrival', 15: 'must', 16: 'lufthansa', 17: 'ground', 18: 'fine', 19: 'requesting', 20: '281', 21: '305', 22: 'weekdays', 23: 'jfk', 24: 'business', 25: 'co', 26: '419', 27: 'has', 28: 'fare', 29: 'have', 30: 'maximum', 31: 'abbreviations', 32: 'define', 33: 'fn', 34: '320', 35: 'nighttime', 36: 'fares', 37: 'arrange', 38: 'arrive', 39: 'today', 40: 'up', 41: '428', 42: 'land', 43: 'later', 44: 'nights', 45: 'traveling', 46: 'off', 47: 'connects', 48: 'will', 49: 'come', 50: 'mondays', 51: 'good', 52: 'else', 53: 'basis', 54: '12', 55: '1230', 56: 'thanks', 57: 'twenty', 58: '311', 59: 'air', 60: 'new', 61: 'indiana', 62: 'jet', 63: 'd9s', 64: 'round', 65: 'meaning', 66: 'like', 67: '217', 68: 'there', 69: 'dh8', 70: 'serviced', 71: '416', 72: 'more', 73: 'afterwards', 74: 'included', 75: 'am', 76: 'meals', 77: 'show', 78: 'trying', 79: 'transport', 80: 'called', 81: 'tower', 82: 'into', 83: 'spend', 84: 'besides', 85: 'highest', 86: 'plan', 87: 'field', 88: 'name', 89: 'if', 90: \"'m\", 91: 'i', 92: 'cheapest', 93: 'through', 94: 'anywhere', 95: 'cheap', 96: 'atl', 97: 'preferably', 98: 'what', 99: 'as', 100: 'nineteenth', 101: '1100', 102: 'february', 103: 'seventeenth', 104: 'diego', 105: 'kansas', 106: 'dinner', 107: 'f28', 108: 'washington', 109: '530', 110: '838', 111: 'eleventh', 112: 'yyz', 113: 'taxi', 114: 'petersburg', 115: 'america', 116: 'month', 117: 'flights', 118: 'economy', 119: 'delta', 120: '1500', 121: 'alaska', 122: 'stand', 123: 'options', 124: 'run', 125: 'los', 126: 'restrictions', 127: '345', 128: 'okay', 129: 'intercontinental', 130: 'd10', 131: 'same', 132: 'smallest', 133: 'seating', 134: 'visit', 135: 'nw', 136: 'boston', 137: '497766', 138: 'sam', 139: '110', 140: 'them', 141: 'town', 142: 'j31', 143: '1039', 144: 'each', 145: 'fourth', 146: 'daily', 147: 'being', 148: '639', 149: 'heading', 150: \"'s\", 151: '1765', 152: 'both', 153: 'thing', 154: 'do', 155: '852', 156: 'still', 157: 'boeing', 158: 'city', 159: 'includes', 160: 'two', 161: '80', 162: 'most', 163: '106', 164: '459', 165: 'connect', 166: 'restriction', 167: 'looking', 168: 'michigan', 169: 'rate', 170: 'whether', 171: 'stopovers', 172: 'need', 173: 'you', 174: 'using', 175: 'july', 176: 'coach', 177: 'got', 178: 'times', 179: '324', 180: 'saturday', 181: '1024', 182: 'types', 183: 'twelfth', 184: 'anything', 185: 'they', 186: 'qualify', 187: 'local', 188: 'stop', 189: 'these', 190: 'kindly', 191: 'hp', 192: 'sundays', 193: 'way', 194: 'prefer', 195: 'california', 196: '1222', 197: 'include', 198: 'makes', 199: 'continental', 200: 'home', 201: 'vegas', 202: 'seventeen', 203: '1291', 204: 'airfare', 205: 'for', 206: 'fridays', 207: 'connections', 208: 'capacities', 209: 'provide', 210: '755', 211: 'equal', 212: 'sometime', 213: 'possible', 214: 'bur', 215: 'some', 216: 'hi', 217: 'northwest', 218: '124', 219: 'very', 220: '771', 221: 'limousine', 222: 'serves', 223: 'leaves', 224: 'fort', 225: 'plane', 226: '630', 227: '1850', 228: 'used', 229: 'departing', 230: 'airplanes', 231: 'wednesdays', 232: 'fly', 233: 'mci', 234: 'please', 235: 'prices', 236: 'l1011', 237: 'orlando', 238: 'expensive', 239: 'layover', 240: 'when', 241: '1220', 242: 'dinnertime', 243: 'see', 244: 'lake', 245: 'reverse', 246: 'hopefully', 247: 'kinds', 248: 'originating', 249: '20', 250: 'largest', 251: 'less', 252: '257', 253: 'sounds', 254: 'leave', 255: '705', 256: 'rental', 257: 'logan', 258: 'downtown', 259: '130', 260: 'fourteenth', 261: 'san', 262: 'closest', 263: 'sfo', 264: 'time', 265: 'uses', 266: 'latest', 267: 'thirty', 268: 'use', 269: 'last', 270: 'scenario', 271: 'dl', 272: 'but', 273: '1300', 274: 'breakfast', 275: 'want', 276: '1600', 277: 'list', 278: 'earlier', 279: 'india', 280: 'codes', 281: 'depart', 282: 'wants', 283: 'flying', 284: 'return', 285: 'stapleton', 286: 'q', 287: 'thrift', 288: 'dollars', 289: 'arrives', 290: 'sixteen', 291: 'pennsylvania', 292: 'philly', 293: 'those', 294: 'qx', 295: 'trips', 296: 'say', 297: 'nashville', 298: \"'ve\", 299: 'days', 300: 'dulles', 301: 'then', 302: 'go', 303: 'should', 304: 'eye', 305: 'worth', 306: \"'ll\", 307: 'francisco', 308: 'in', 309: 'booking', 310: 'with', 311: '82', 312: 'us', 313: 'longest', 314: '718', 315: 'indianapolis', 316: 'out', 317: 'only', 318: 'a.m.', 319: 'rates', 320: 'nonstop', 321: 'number', 322: 'sure', 323: 'schedules', 324: 'seat', 325: 'transcontinental', 326: 'hello', 327: 'june', 328: 'fifteen', 329: \"'re\", 330: 'canada', 331: '0900', 332: 'listing', 333: 'canadian', 334: 'baltimore', 335: 'listed', 336: 'pittsburgh', 337: 'planning', 338: 'departures', 339: 'that', 340: 'book', 341: '72s', 342: '1993', 343: 'meal', 344: 'services', 345: 'quebec', 346: '296', 347: 'six', 348: '11', 349: '845', 350: 'night', 351: 'could', 352: 'continent', 353: 'ap57', 354: 'wn', 355: 'reaches', 356: 'working', 357: 'evening', 358: 'tuesday', 359: 'choices', 360: '430', 361: 'limo', 362: 'destination', 363: 'yes', 364: 'ohio', 365: 'far', 366: '1110', 367: '8', 368: 'between', 369: 'september', 370: 'noon', 371: 'october', 372: '382', 373: 'ord', 374: 'montreal', 375: 'ua', 376: 'southwest', 377: 'at', 378: 'cars', 379: 'snack', 380: 'about', 381: 'advertises', 382: '297', 383: 'ten', 384: 'oak', 385: 'ff', 386: 'coming', 387: 'cities', 388: 'chicago', 389: 'airports', 390: 'determine', 391: '150', 392: 'thirteenth', 393: '645', 394: 'transportation', 395: 'actually', 396: 'offers', 397: 'economic', 398: 'florida', 399: '6', 400: 'trans', 401: 'departs', 402: 'much', 403: 'march', 404: 'sixth', 405: '1245', 406: 'tell', 407: '9', 408: 'level', 409: 'supper', 410: 'companies', 411: 'fit', 412: 'route', 413: 'catch', 414: 'schedule', 415: '3724', 416: 'third', 417: 'car', 418: 'tenth', 419: 'again', 420: 'proper', 421: 'distance', 422: '57', 423: 'bna', 424: 'instead', 425: 'wish', 426: 'oakland', 427: 'zone', 428: 'noontime', 429: 'numbers', 430: 'least', 431: 'twentieth', 432: 'bring', 433: 'west', 434: 'tickets', 435: '420', 436: 'me', 437: 'early', 438: 'midwest', 439: 'angeles', 440: 'find', 441: 'greatest', 442: 'iah', 443: 'milwaukee', 444: 'including', 445: 'having', 446: 'tuesdays', 447: 'classes', 448: 'any', 449: 'service', 450: 'repeat', 451: 'would', 452: 'pm', 453: 'departure', 454: 'get', 455: 'seven', 456: 'take', 457: '3', 458: 'charlotte', 459: '737', 460: 'nevada', 461: 'interested', 462: 'carried', 463: '1209', 464: 'louis', 465: 'january', 466: 'train', 467: 'sixteenth', 468: 'ap', 469: 'itinerary', 470: '486', 471: '932', 472: 'final', 473: 'oh', 474: '1145', 475: 'soon', 476: 'which', 477: 'airlines', 478: 'available', 479: 'along', 480: 'airline', 481: 'one', 482: 'salt', 483: 'reaching', 484: 'great', 485: 'ninth', 486: '225', 487: '2100', 488: 'united', 489: 'stopover', 490: 'la', 491: 'tacoma', 492: 'minneapolis', 493: 'airplane', 494: '271', 495: 'begins', 496: 'bh', 497: 'carries', 498: 'hold', 499: 'arrangements', 500: '139', 501: '10', 502: 'seattle', 503: 'it', 504: 'f', 505: 'lives', 506: '4400', 507: 'airport', 508: 'international', 509: 'ewr', 510: 'friday', 511: 'y', 512: '555', 513: 'going', 514: 'now', 515: 'taking', 516: 'reservation', 517: 'toward', 518: 'sorry', 519: 'across', 520: 'stands', 521: 'give', 522: 'straight', 523: 'travel', 524: 'connecting', 525: '734', 526: 'utah', 527: '71', 528: 'served', 529: 'mornings', 530: 'my', 531: 'lax', 532: 'close', 533: 'seats', 534: 'snacks', 535: '300', 536: '1', 537: '767', 538: 'shortest', 539: 'rent', 540: 'let', 541: 'qo', 542: 'inexpensive', 543: '747', 544: 'other', 545: '1055', 546: 'm80', 547: 'be', 548: 'well', 549: 'dfw', 550: '210', 551: 'where', 552: '1059', 553: 'offer', 554: 'after', 555: 'sunday', 556: 'friends', 557: 'miles', 558: 'calling', 559: 'before', 560: '3357', 561: 'staying', 562: '505', 563: 'landings', 564: 'lands', 565: 'near', 566: '1991', 567: 'not', 568: '723', 569: 'kind', 570: 'paul', 571: 'monday', 572: 'week', 573: 'tomorrow', 574: \"'d\", 575: 'ap68', 576: 'mean', 577: 'morning', 578: 'planes', 579: 'starting', 580: 'afternoon', 581: 'takeoff', 582: 'seventh', 583: 'mco', 584: '1030', 585: 'three', 586: 'amount', 587: 'takeoffs', 588: 'flies', 589: '2134', 590: 'live', 591: 'qw', 592: 'put', 593: 'american', 594: 'be1', 595: 'dc10', 596: '1000', 597: 'eighteenth', 598: 'of', 599: 'explain', 600: 'midnight', 601: 'try', 602: 'their', 603: '1020', 604: 'costs', 605: 'columbus', 606: 'reservations', 607: 'rentals', 608: 'thursday', 609: 'via', 610: '1992', 611: 'lastest', 612: 'no', 613: 'hours', 614: '466', 615: 'tonight', 616: 'or', 617: 'abbreviation', 618: 'weekday', 619: 'overnight', 620: '1700', 621: 'minnesota', 622: 'grounds', 623: 'various', 624: 'such', 625: 'thirtieth', 626: 'summer', 627: 'are', 628: 'jose', 629: 'second', 630: 'know', 631: 'on', 632: 'kennedy', 633: \"o'clock\", 634: 'mitchell', 635: '1505', 636: '757', 637: 'how', 638: '823', 639: 'stops', 640: '73s', 641: 'guardia', 642: 's', 643: 'exceeding', 644: 'lga', 645: 'york', 646: '746', 647: 'county', 648: 'back', 649: '1017', 650: 'lunch', 651: 'aa', 652: 'direct', 653: 'who', 654: '163', 655: 'over', 656: 'following', 657: 'total', 658: 'single', 659: 'miami', 660: 'red', 661: 'august', 662: '100', 663: '201', 664: 'travels', 665: 'your', 666: 'long', 667: 'inform', 668: 'k', 669: 'discount', 670: 'may', 671: 'landing', 672: 'flight', 673: 'belong', 674: 'under', 675: 'thank', 676: 'limousines', 677: 'priced', 678: 'directly', 679: 'world', 680: 'first', 681: 'regarding', 682: 'north', 683: 'ac', 684: 'hou', 685: 'april', 686: 'day', 687: 'arriving', 688: 'midway', 689: 'the', 690: 'goes', 691: 'by', 692: '315', 693: 'this', 694: 'can', 695: '1207', 696: '329', 697: 'twa', 698: 'pearson', 699: 'class', 700: 'turboprop', 701: 'represented', 702: 'too', 703: 'texas', 704: 'approximately', 705: 'ontario', 706: 'does', 707: 'mealtime', 708: 'right', 709: '1940', 710: 'usa', 711: '405', 712: 'than', 713: '4', 714: 'atlanta', 715: '2', 716: '720', 717: '813', 718: 'designate', 719: 'while', 720: 'thursdays', 721: 'a', 722: 'question', 723: 'symbols', 724: 'burbank', 725: 'cleveland', 726: 'las', 727: 'order', 728: 'from', 729: 'display', 730: 'ticket', 731: 'provided', 732: '1205', 733: 'and', 734: 'charges', 735: 'scheduled', 736: 'area', 737: 'beginning', 738: 'beach', 739: 'eight', 740: 'connection', 741: '21', 742: 'offered', 743: 'cost', 744: 'stopping', 745: 'price', 746: 'memphis', 747: 'located', 748: 'minimum', 749: 'houston', 750: 'dc', 751: 'yn', 752: 'st.', 753: 'ea', 754: 'type', 755: 'information', 756: 'general', 757: 'l10', 758: '269', 759: 'names', 760: 'also', 761: 'difference', 762: '539', 763: '934', 764: '1130', 765: 'tampa', 766: 'returning', 767: '212', 768: 'ever', 769: 'missouri', 770: '230', 771: 'leaving', 772: 'database', 773: 'ls', 774: 'another', 775: 'cvg', 776: 'make', 777: '1800', 778: 'help', 779: 'saturdays', 780: 'is', 781: 'here', 782: 'eastern', 783: 'next', 784: 'detroit', 785: 'fifteenth', 786: 'thereafter', 787: '343', 788: 'dca', 789: 'toronto', 790: 'operation', 791: 'we', 792: 'many', 793: 'bound', 794: '417', 795: 'code', 796: 'bwi', 797: 'arrivals', 798: 'm', 799: 'b', 800: 'able', 801: '400', 802: 'georgia', 803: 'kw', 804: 'bay', 805: 'sort', 806: 'without', 807: 'love', 808: 'westchester', 809: 'somebody', 810: \"n't\", 811: 'lowest', 812: 'arizona', 813: 'describe', 814: 'colorado', 815: 'express', 816: '727', 817: 'nationair', 818: 'tennessee', 819: 'so', 820: 'continuing', 821: 'people', 822: 'serving', 823: '1158', 824: 'philadelphia', 825: '733', 826: '608', 827: 'h', 828: '5', 829: 'comes', 830: 'different', 831: 'capacity', 832: '1045', 833: 'wednesday', 834: 'making', 835: '825', 836: 'fifth', 837: 'jersey', 838: 'aircraft', 839: 'newark', 840: '200', 841: 'late', 842: 'look', 843: '1115', 844: 'denver', 845: '730', 846: 'all', 847: 'cincinnati', 848: 'earliest', 849: 'within', 850: 'phoenix', 851: '7', 852: 'ap80', 853: 'vicinity', 854: 'nonstops', 855: 'equipment', 856: 'to', 857: 'passengers', 858: 'carolina', 859: 'december', 860: '270', 861: 'around', 862: 'eighth'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Padding","metadata":{}},{"cell_type":"code","source":"length = (len(seq) for seq in X)\nprint(format(max(length)))","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:46.569078Z","iopub.execute_input":"2023-03-18T12:42:46.569381Z","iopub.status.idle":"2023-03-18T12:42:46.579173Z","shell.execute_reply.started":"2023-03-18T12:42:46.569356Z","shell.execute_reply":"2023-03-18T12:42:46.578220Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"46\n","output_type":"stream"}]},{"cell_type":"code","source":"MAX_SEQ_LENGTH = 46\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nx_padded = pad_sequences(X,maxlen = MAX_SEQ_LENGTH, padding = 'pre', truncating = 'post' )\ny_padded = pad_sequences(Y,maxlen = MAX_SEQ_LENGTH, padding = 'pre', truncating = 'post' )\n","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:46.580602Z","iopub.execute_input":"2023-03-18T12:42:46.580921Z","iopub.status.idle":"2023-03-18T12:42:55.289020Z","shell.execute_reply.started":"2023-03-18T12:42:46.580891Z","shell.execute_reply":"2023-03-18T12:42:55.287753Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print(x_padded[0])","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:55.290350Z","iopub.execute_input":"2023-03-18T12:42:55.291737Z","iopub.status.idle":"2023-03-18T12:42:55.298816Z","shell.execute_reply.started":"2023-03-18T12:42:55.291688Z","shell.execute_reply":"2023-03-18T12:42:55.298104Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0  98 780 689 743 598 721  64  11 672 728 336 856 714\n 737 631 685  57 836 733 766 631 670 404]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(torch.from_numpy(np.array(x_padded[0])))","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:55.299784Z","iopub.execute_input":"2023-03-18T12:42:55.300641Z","iopub.status.idle":"2023-03-18T12:42:55.346123Z","shell.execute_reply.started":"2023-03-18T12:42:55.300613Z","shell.execute_reply":"2023-03-18T12:42:55.344687Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,  98, 780, 689, 743, 598,\n        721,  64,  11, 672, 728, 336, 856, 714, 737, 631, 685,  57, 836, 733,\n        766, 631, 670, 404], dtype=torch.int32)\n","output_type":"stream"}]},{"cell_type":"code","source":"#  conversion done here to convert it into a proper training data set.\ntraining_data = []\nfor i in range(num_sentences):\n    training_data.append((X[i],Y[i]))\n    \ndev_data = []\nfor i in range(num_sentences_dev):\n    dev_data.append((X_dev[i],Y_dev[i]))\n    \ntest_data = []\nfor i in range(num_sentences_test):\n    test_data.append((X_test[i],Y_test[i]))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:55.347806Z","iopub.execute_input":"2023-03-18T12:42:55.348178Z","iopub.status.idle":"2023-03-18T12:42:55.358434Z","shell.execute_reply.started":"2023-03-18T12:42:55.348144Z","shell.execute_reply":"2023-03-18T12:42:55.357223Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(\"d\")\nprint(test_data[0])","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:55.359697Z","iopub.execute_input":"2023-03-18T12:42:55.360080Z","iopub.status.idle":"2023-03-18T12:42:55.368300Z","shell.execute_reply.started":"2023-03-18T12:42:55.360051Z","shell.execute_reply":"2023-03-18T12:42:55.367087Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"d\n([98, 627, 689, 176, 117, 368, 4, 733, 334, 771, 661, 418, 733, 766, 661, 0], [11, 6, 12, 5, 5, 4, 7, 10, 7, 8, 5, 2, 10, 8, 5, 9])\n","output_type":"stream"}]},{"cell_type":"code","source":"# # training sentences and their corresponding word-tags\n# training_data = [\n#     (\"The cat ate the cheese\".lower().split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n#     (\"She read that book\".lower().split(), [\"NN\", \"V\", \"DET\", \"NN\"]),\n#     (\"The dog loves art\".lower().split(), [\"DET\", \"NN\", \"V\", \"NN\"]),\n#     (\"The elephant answers the phone\".lower().split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"])\n# ]\n\n# # create a dictionary that maps words to indices\n# word2idx = {}\n\n# for sent, tags in training_data:\n    \n#     for word in sent:\n        \n#         if word not in word2idx:\n            \n#             word2idx[word] = len(word2idx)\n\n# # create a dictionary that maps tags to indices\n# tag2idx = {\"DET\": 0, \"NN\": 1, \"V\": 2}","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:55.369787Z","iopub.execute_input":"2023-03-18T12:42:55.370166Z","iopub.status.idle":"2023-03-18T12:42:55.380828Z","shell.execute_reply.started":"2023-03-18T12:42:55.370129Z","shell.execute_reply":"2023-03-18T12:42:55.380021Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# # print out the created dictionary\n# print(word2idx)\n# print(tag2idx)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:55.382098Z","iopub.execute_input":"2023-03-18T12:42:55.382410Z","iopub.status.idle":"2023-03-18T12:42:55.389218Z","shell.execute_reply.started":"2023-03-18T12:42:55.382376Z","shell.execute_reply":"2023-03-18T12:42:55.388258Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n\n# # a helper function for converting a sequence of words to a Tensor of numerical values\n# # will be used later in training\n# def prepare_sequence(seq, to_idx):\n#     '''This function takes in a sequence of words and returns a \n#     corresponding Tensor of numerical values (indices for each word).'''\n    \n#     idxs = [to_idx[w] for w in seq]\n#     idxs = np.array(idxs)\n    \n#     return torch.from_numpy(idxs)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:55.390746Z","iopub.execute_input":"2023-03-18T12:42:55.391241Z","iopub.status.idle":"2023-03-18T12:42:55.400696Z","shell.execute_reply.started":"2023-03-18T12:42:55.391201Z","shell.execute_reply":"2023-03-18T12:42:55.399664Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# # check out what prepare_sequence does for one of our training sentences:\n# example_input = prepare_sequence(\"The dog answers the phone\".lower().split(), word2idx)\n\n# print(example_input)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:55.402114Z","iopub.execute_input":"2023-03-18T12:42:55.402549Z","iopub.status.idle":"2023-03-18T12:42:55.409235Z","shell.execute_reply.started":"2023-03-18T12:42:55.402516Z","shell.execute_reply":"2023-03-18T12:42:55.408398Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Creating the Model","metadata":{}},{"cell_type":"code","source":"class LSTMTagger(nn.Module):\n\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n        ''' Initialize the layers of this model.'''\n        super(LSTMTagger, self).__init__()\n        \n        self.hidden_dim = hidden_dim\n\n        # embedding layer that turns words into a vector of a specified size\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n\n        # the LSTM takes embedded word vectors (of a specified size) as inputs \n        # and outputs hidden states of size hidden_dim\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n\n        # the linear layer that maps the hidden state output dimension \n        # to the number of tags we want as output, tagset_size (in this case this is 3 tags)\n        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n        \n        # initialize the hidden state (see code below)\n        self.hidden = self.init_hidden()\n\n        \n    def init_hidden(self):\n        ''' At the start of training, we need to initialize a hidden state;\n           there will be none because the hidden state is formed based on perviously seen data.\n           So, this function defines a hidden state with all zeroes and of a specified size.'''\n        # The axes dimensions are (n_layers, batch_size, hidden_dim)\n        return (torch.zeros(1, 1, self.hidden_dim),\n                torch.zeros(1, 1, self.hidden_dim))\n\n    def forward(self, sentence):\n        ''' Define the feedforward behavior of the model.'''\n        # create embedded word vectors for each word in a sentence\n        embeds = self.word_embeddings(sentence)\n        \n        # get the output and hidden state by passing the lstm over our word embeddings\n        # the lstm takes in our embeddings and hiddent state\n        lstm_out, self.hidden = self.lstm(\n            embeds.view(len(sentence), 1, -1), self.hidden)\n        \n        # get the scores for the most likely tag for a word\n        tag_outputs = self.hidden2tag(lstm_out.view(len(sentence), -1))\n        tag_scores = F.log_softmax(tag_outputs, dim=1)\n        \n        return tag_scores","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:55.410545Z","iopub.execute_input":"2023-03-18T12:42:55.410894Z","iopub.status.idle":"2023-03-18T12:42:55.421186Z","shell.execute_reply.started":"2023-03-18T12:42:55.410864Z","shell.execute_reply":"2023-03-18T12:42:55.419882Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Define how the model trains ","metadata":{}},{"cell_type":"code","source":"# the embedding dimension defines the size of our word vectors\n# for our simple vocabulary and training set, we will keep these small\nEMBEDDING_DIM = 300\nHIDDEN_DIM = 300\n\n# instantiate our model\nmodel = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(index), num_tags)\n\n# define our loss and optimizer\nloss_function = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:55.422211Z","iopub.execute_input":"2023-03-18T12:42:55.422524Z","iopub.status.idle":"2023-03-18T12:42:55.474116Z","shell.execute_reply.started":"2023-03-18T12:42:55.422495Z","shell.execute_reply":"2023-03-18T12:42:55.473224Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# test_sentence = \"The cheese loves the elephant\".lower().split()\n\n# # see what the scores are before training\n# # element [i,j] of the output is the *score* for tag j for word i.\n# # to check the initial accuracy of our model, we don't need to train, so we use model.eval()\n# inputs = prepare_sequence(test_sentence, word2idx)\n# inputs = inputs\n# print(inputs)\n# tag_scores = model(inputs)\n# print(tag_scores)\n\n# # tag_scores outputs a vector of tag scores for each word in an inpit sentence\n# # to get the most likely tag index, we grab the index with the maximum score!\n# # recall that these numbers correspond to tag2idx = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n# _, predicted_tags = torch.max(tag_scores, 1)\n# print('\\n')\n# print('Predicted tags: \\n',predicted_tags)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:55.475459Z","iopub.execute_input":"2023-03-18T12:42:55.476350Z","iopub.status.idle":"2023-03-18T12:42:55.481882Z","shell.execute_reply.started":"2023-03-18T12:42:55.476287Z","shell.execute_reply":"2023-03-18T12:42:55.480066Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Train the Model","metadata":{}},{"cell_type":"code","source":"import random\nfrom sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:55.483729Z","iopub.execute_input":"2023-03-18T12:42:55.484403Z","iopub.status.idle":"2023-03-18T12:42:55.769273Z","shell.execute_reply.started":"2023-03-18T12:42:55.484364Z","shell.execute_reply":"2023-03-18T12:42:55.768195Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"    label = []\n    for i in range(len(idx_to_word_tag)):\n        label.append(idx_to_word_tag[i])\n    print(label)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:55.770477Z","iopub.execute_input":"2023-03-18T12:42:55.770847Z","iopub.status.idle":"2023-03-18T12:42:55.777992Z","shell.execute_reply.started":"2023-03-18T12:42:55.770814Z","shell.execute_reply":"2023-03-18T12:42:55.776726Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"['ADV', 'PART', 'ADJ', 'INTJ', 'ADP', 'NOUN', 'AUX', 'PROPN', 'VERB', 'NUM', 'CCONJ', 'PRON', 'DET']\n","output_type":"stream"}]},{"cell_type":"code","source":"n_epochs = 10\nbatch_size = 20\n\nfor epoch in range(n_epochs):\n    \n    epoch_loss = torch.tensor(0, dtype=torch.float) \n    total_length_accuracy = 0\n    correct_length_accuracy = 0\n    # shuffle the training data\n    random.shuffle(training_data)\n    batches = []\n    # split the training data into batches\n    for batch_start in range(0, len(training_data), batch_size):\n        batch_end = batch_start + batch_size\n        batch_sentences = [data[0] for data in training_data[batch_start:batch_end]]\n        batch_tags = [data[1] for data in training_data[batch_start:batch_end]]\n        \n        batches.append(training_data[batch_start:batch_end])\n#     print(batches)\n    for batch in batches:\n        total_loss = torch.tensor(0, dtype=torch.float) \n        \n        # initialize total_loss as a PyTorch tensor object        total_length_accuracy  = 0 \n#         correct_length_accuracy  = 0 \n        # zero the gradients\n        model.zero_grad()\n\n        # zero the hidden state of the LSTM, this detaches it from its history\n        model.hidden = model.init_hidden()\n \n        # prepare the inputs for processing by out network, \n        # turn all sentences and targets into Tensors of numerical indices\n        for sentence, tags in batch:\n#             print(sentence)\n            targets = torch.from_numpy(np.array(tags))\n#             print(targets[1])\n            # forward pass to get tag scores\n            tag_scores = model(torch.from_numpy(np.array(sentence)))\n#         print(tag_scores)\n            # compute the loss, and gradients \n            loss = loss_function(tag_scores, targets)\n            epoch_loss += loss\n            total_loss += loss\n            total_length_accuracy+=len(tag_scores)\n#             tag_scores = tag_scores.item()\n#             targets = targets.item()\n            _, predicted_tags = torch.max(tag_scores, 1)\n            for i in range(len(tag_scores)):\n#                 print(predicted_tags[i], targets[i])\n\n                if predicted_tags[i] == targets[i]:\n                    correct_length_accuracy+=1\n                    \n#         print(\"necttttt\")\n#         print(correct_length_accuracy, total_length_accuracy)\n        # compute the loss, and gradients \n        total_loss.backward()\n        \n        # update the model parameters with optimizer.step()\n        optimizer.step()\n        \n#   now we have to check accuracy \n#   1. accuracy on training data\n    accuracy_train = correct_length_accuracy/total_length_accuracy\n    \n#   2. accuracy on dev data set and test\n    total_length_dev = 0 \n    correct_length_dev = 0\n    total_length_test = 0\n    correct_length_test = 0\n        \n    for sentence, tags in dev_data:\n        targets = torch.from_numpy(np.array(tags))\n        tag_scores = model(torch.from_numpy(np.array(sentence)))\n        _, predicted_tags = torch.max(tag_scores, 1)\n        for i in range(len(tag_scores)):\n            if predicted_tags[i] == targets[i]:\n                correct_length_dev += 1\n        total_length_dev += len(tag_scores)\n    tags_orig = []\n    tags_pred = []\n    for sentence, tags in test_data:\n        targets = torch.from_numpy(np.array(tags))\n        for tag in tags:\n            tags_orig.append(tag)\n        tag_scores = model(torch.from_numpy(np.array(sentence)))\n        _, predicted_tags = torch.max(tag_scores, 1)\n        pred_tagu = predicted_tags.tolist()\n        for tag in pred_tagu:\n            tags_pred.append(tag)\n        for i in range(len(tag_scores)):\n            if predicted_tags[i] == targets[i]:\n                correct_length_test += 1\n        total_length_test += len(tag_scores)\n            \n        accuracy_dev = correct_length_dev/total_length_dev\n        accuracy_test = correct_length_test/total_length_test\n#     print(tags_orig, tags_pred)\n#     label = []\n#     for i in range(len(idx_to_word_tag)):\n#         label.append()\n    print(classification_report(tags_orig, tags_pred, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]))\n    # print out avg loss per epoch\n    print(\"Epoch: %d, loss: %1.5f, Train_Acc : %1.5f, dev_acc : %1.5f, test_acc : %1.5f\" % (epoch+1, epoch_loss.item()/len(training_data),accuracy_train, accuracy_dev, accuracy_test))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:42:55.779158Z","iopub.execute_input":"2023-03-18T12:42:55.779474Z","iopub.status.idle":"2023-03-18T12:49:40.656237Z","shell.execute_reply.started":"2023-03-18T12:42:55.779441Z","shell.execute_reply":"2023-03-18T12:49:40.655246Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.90      0.80      0.85        76\n           1       0.98      0.96      0.97        56\n           2       0.95      0.94      0.94       220\n           3       1.00      1.00      1.00        36\n           4       1.00      1.00      1.00      1434\n           5       0.99      0.99      0.99      1166\n           6       0.93      0.98      0.96       256\n           7       0.99      0.99      0.99      1567\n           8       0.98      0.96      0.97       629\n           9       0.91      0.99      0.95       127\n          10       1.00      0.99      1.00       109\n          11       0.96      0.76      0.85       392\n          12       0.84      0.97      0.90       512\n\n    accuracy                           0.97      6580\n   macro avg       0.96      0.95      0.95      6580\nweighted avg       0.97      0.97      0.97      6580\n\nEpoch: 1, loss: 0.16160, Train_Acc : 0.94720, dev_acc : 0.96742, test_acc : 0.96854\n              precision    recall  f1-score   support\n\n           0       0.97      0.74      0.84        76\n           1       0.98      0.98      0.98        56\n           2       0.92      0.97      0.95       220\n           3       1.00      1.00      1.00        36\n           4       1.00      1.00      1.00      1434\n           5       0.99      0.99      0.99      1166\n           6       0.96      0.93      0.95       256\n           7       0.99      0.99      0.99      1567\n           8       0.97      0.97      0.97       629\n           9       0.90      1.00      0.95       127\n          10       1.00      1.00      1.00       109\n          11       0.85      0.98      0.91       392\n          12       0.98      0.87      0.92       512\n\n    accuracy                           0.98      6580\n   macro avg       0.96      0.96      0.96      6580\nweighted avg       0.98      0.98      0.98      6580\n\nEpoch: 2, loss: 0.06314, Train_Acc : 0.97408, dev_acc : 0.97496, test_acc : 0.97538\n              precision    recall  f1-score   support\n\n           0       0.92      0.86      0.88        76\n           1       0.98      0.96      0.97        56\n           2       0.96      0.95      0.95       220\n           3       1.00      1.00      1.00        36\n           4       1.00      1.00      1.00      1434\n           5       0.99      0.99      0.99      1166\n           6       0.96      0.96      0.96       256\n           7       0.99      1.00      0.99      1567\n           8       0.99      0.97      0.98       629\n           9       0.94      0.97      0.95       127\n          10       1.00      0.99      1.00       109\n          11       0.85      0.99      0.91       392\n          12       0.99      0.86      0.92       512\n\n    accuracy                           0.98      6580\n   macro avg       0.97      0.96      0.96      6580\nweighted avg       0.98      0.98      0.98      6580\n\nEpoch: 3, loss: 0.05434, Train_Acc : 0.97616, dev_acc : 0.97240, test_acc : 0.97660\n              precision    recall  f1-score   support\n\n           0       0.93      0.84      0.88        76\n           1       0.98      0.98      0.98        56\n           2       0.96      0.95      0.96       220\n           3       1.00      1.00      1.00        36\n           4       1.00      1.00      1.00      1434\n           5       0.99      0.99      0.99      1166\n           6       0.95      0.96      0.96       256\n           7       0.99      1.00      0.99      1567\n           8       0.98      0.97      0.97       629\n           9       0.91      0.98      0.94       127\n          10       1.00      1.00      1.00       109\n          11       0.88      0.92      0.90       392\n          12       0.93      0.90      0.92       512\n\n    accuracy                           0.98      6580\n   macro avg       0.96      0.96      0.96      6580\nweighted avg       0.98      0.98      0.98      6580\n\nEpoch: 4, loss: 0.05325, Train_Acc : 0.97638, dev_acc : 0.97285, test_acc : 0.97660\n              precision    recall  f1-score   support\n\n           0       0.88      0.87      0.87        76\n           1       0.98      0.96      0.97        56\n           2       0.96      0.95      0.95       220\n           3       1.00      1.00      1.00        36\n           4       1.00      1.00      1.00      1434\n           5       0.99      0.99      0.99      1166\n           6       0.95      0.97      0.96       256\n           7       0.99      0.99      0.99      1567\n           8       0.99      0.97      0.98       629\n           9       0.90      0.98      0.94       127\n          10       1.00      1.00      1.00       109\n          11       0.87      0.96      0.91       392\n          12       0.97      0.88      0.93       512\n\n    accuracy                           0.98      6580\n   macro avg       0.96      0.96      0.96      6580\nweighted avg       0.98      0.98      0.98      6580\n\nEpoch: 5, loss: 0.05151, Train_Acc : 0.97719, dev_acc : 0.97210, test_acc : 0.97720\n              precision    recall  f1-score   support\n\n           0       0.93      0.74      0.82        76\n           1       0.98      0.96      0.97        56\n           2       0.72      0.95      0.82       220\n           3       1.00      0.97      0.99        36\n           4       1.00      1.00      1.00      1434\n           5       0.99      0.99      0.99      1166\n           6       0.98      0.93      0.95       256\n           7       0.99      0.99      0.99      1567\n           8       0.97      0.88      0.92       629\n           9       0.88      0.99      0.93       127\n          10       1.00      1.00      1.00       109\n          11       0.85      0.99      0.91       392\n          12       0.99      0.87      0.92       512\n\n    accuracy                           0.97      6580\n   macro avg       0.94      0.94      0.94      6580\nweighted avg       0.97      0.97      0.97      6580\n\nEpoch: 6, loss: 0.05186, Train_Acc : 0.97737, dev_acc : 0.96350, test_acc : 0.96565\n              precision    recall  f1-score   support\n\n           0       0.91      0.84      0.88        76\n           1       1.00      0.82      0.90        56\n           2       0.95      0.95      0.95       220\n           3       1.00      0.89      0.94        36\n           4       0.99      0.99      0.99      1434\n           5       0.99      0.99      0.99      1166\n           6       0.97      0.95      0.96       256\n           7       0.99      1.00      0.99      1567\n           8       0.97      0.97      0.97       629\n           9       0.93      0.99      0.96       127\n          10       1.00      1.00      1.00       109\n          11       0.92      0.81      0.86       392\n          12       0.86      0.96      0.91       512\n\n    accuracy                           0.97      6580\n   macro avg       0.96      0.94      0.95      6580\nweighted avg       0.97      0.97      0.97      6580\n\nEpoch: 7, loss: 0.05139, Train_Acc : 0.97784, dev_acc : 0.96833, test_acc : 0.97067\n              precision    recall  f1-score   support\n\n           0       0.89      0.76      0.82        76\n           1       0.98      0.96      0.97        56\n           2       0.93      0.94      0.93       220\n           3       1.00      1.00      1.00        36\n           4       1.00      0.99      1.00      1434\n           5       0.99      0.99      0.99      1166\n           6       0.96      0.95      0.96       256\n           7       0.99      0.99      0.99      1567\n           8       0.98      0.96      0.97       629\n           9       0.88      0.98      0.93       127\n          10       1.00      1.00      1.00       109\n          11       0.88      0.92      0.90       392\n          12       0.93      0.90      0.91       512\n\n    accuracy                           0.97      6580\n   macro avg       0.95      0.95      0.95      6580\nweighted avg       0.97      0.97      0.97      6580\n\nEpoch: 8, loss: 0.04932, Train_Acc : 0.97848, dev_acc : 0.97330, test_acc : 0.97204\n              precision    recall  f1-score   support\n\n           0       0.94      0.83      0.88        76\n           1       0.98      0.96      0.97        56\n           2       0.95      0.95      0.95       220\n           3       1.00      0.92      0.96        36\n           4       1.00      1.00      1.00      1434\n           5       0.99      0.99      0.99      1166\n           6       0.95      0.97      0.96       256\n           7       0.99      0.99      0.99      1567\n           8       0.98      0.96      0.97       629\n           9       0.84      0.99      0.91       127\n          10       1.00      1.00      1.00       109\n          11       0.86      0.95      0.91       392\n          12       0.96      0.88      0.92       512\n\n    accuracy                           0.97      6580\n   macro avg       0.96      0.95      0.96      6580\nweighted avg       0.98      0.97      0.97      6580\n\nEpoch: 9, loss: 0.05103, Train_Acc : 0.97793, dev_acc : 0.97164, test_acc : 0.97477\n              precision    recall  f1-score   support\n\n           0       0.90      0.70      0.79        76\n           1       0.98      0.96      0.97        56\n           2       0.92      0.96      0.94       220\n           3       1.00      1.00      1.00        36\n           4       0.99      1.00      1.00      1434\n           5       0.99      0.99      0.99      1166\n           6       0.96      0.95      0.96       256\n           7       0.99      0.99      0.99      1567\n           8       0.98      0.97      0.98       629\n           9       0.85      0.97      0.91       127\n          10       1.00      0.98      0.99       109\n          11       0.86      0.97      0.91       392\n          12       0.97      0.88      0.92       512\n\n    accuracy                           0.97      6580\n   macro avg       0.95      0.95      0.95      6580\nweighted avg       0.98      0.97      0.97      6580\n\nEpoch: 10, loss: 0.05092, Train_Acc : 0.97797, dev_acc : 0.97436, test_acc : 0.97447\n","output_type":"stream"}]},{"cell_type":"code","source":"# Saving the model\ntorch.save(model.state_dict(), 'model.pt')","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:49:40.657479Z","iopub.execute_input":"2023-03-18T12:49:40.658175Z","iopub.status.idle":"2023-03-18T12:49:40.672538Z","shell.execute_reply.started":"2023-03-18T12:49:40.658147Z","shell.execute_reply":"2023-03-18T12:49:40.671012Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# # normally these epochs take a lot longer \n# # but with our toy data (only 3 sentences), we can do many epochs in a short time\n# n_epochs = 10\n\n# # if torch.cuda.is_available():\n# #     device = torch.device(\"cuda\")\n# # else:\n# #     device = torch.device(\"cpu\")\n\n# # model.to(device)\n\n# for epoch in range(n_epochs):\n    \n#     epoch_loss = 0.0\n    \n#     # get all sentences and corresponding tags in the training data\n#     for sentence, tags in training_data:\n        \n#         # zero the gradients\n#         model.zero_grad()        \n\n#         # zero the hidden state of the LSTM, this detaches it from its history\n#         model.hidden = model.init_hidden()\n\n#         # prepare the inputs for processing by out network, \n#         # turn all sentences and targets into Tensors of numerical indices\n# #         sentence_in = prepare_sequence(sentence, word2idx)\n#         targets = torch.from_numpy(np.array(tags))\n\n#         # forward pass to get tag scores\n#         tag_scores = model(torch.from_numpy(np.array(sentence)))\n# #         print(tag_scores)\n#         # compute the loss, and gradients \n#         loss = loss_function(tag_scores, targets)\n#         epoch_loss += loss.item()\n#         loss.backward()\n        \n#         # update the model parameters with optimizer.step()\n#         optimizer.step()\n        \n#     # print out avg loss per 20 epochs\n# #     if(epoch%1 == 1):\n#     print(\"Epoch: %d, loss: %1.5f\" % (epoch+1, epoch_loss/len(training_data)))","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:49:40.674059Z","iopub.execute_input":"2023-03-18T12:49:40.674913Z","iopub.status.idle":"2023-03-18T12:49:40.679684Z","shell.execute_reply.started":"2023-03-18T12:49:40.674879Z","shell.execute_reply":"2023-03-18T12:49:40.678766Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"test_sentence = \"plane\".lower().split()\n\nword_to_idx = {}\nidx_to_word = {}\n\nunique_words = set(words_total)\n\nfor i, word in enumerate(unique_words):\n    word_to_idx[word] = i\n    idx_to_word[i] = word\n    \nencoded_sent = []\nfor word in test_sentence:\n    encoded_sent.append(word_to_idx[word])\n    \n# print(encoded_sent)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:53:47.628857Z","iopub.execute_input":"2023-03-18T12:53:47.629242Z","iopub.status.idle":"2023-03-18T12:53:47.637513Z","shell.execute_reply.started":"2023-03-18T12:53:47.629209Z","shell.execute_reply":"2023-03-18T12:53:47.636597Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# see what the scores are after training\n# inputs = prepare_sequence(test_sentence, word2idx)\n# inputs = inputs\ntag_scores = model(torch.from_numpy(np.array(encoded_sent)))\n# print(tag_scores)\n\n# print the most likely tag index, by grabbing the index with the maximum score!\n# recall that these numbers correspond to tag2idx = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n_, predicted_tagss = torch.max(tag_scores, 1)\nans = []\ntags = predicted_tagss.tolist()\nfor idx in tags:\n    encoded_sent = idx_to_word_tag[idx]\n    ans.append(encoded_sent)\n    \n# print(idx_to_word_tag)\n# print(ans)\n# print('\\n')\n# print('Predicted tags: \\n',predicted_tagss)\nfor i in range(len(ans)):\n    print(test_sentence[i], \"\\t\", ans[i])","metadata":{"execution":{"iopub.status.busy":"2023-03-18T12:53:50.508251Z","iopub.execute_input":"2023-03-18T12:53:50.509503Z","iopub.status.idle":"2023-03-18T12:53:50.518466Z","shell.execute_reply.started":"2023-03-18T12:53:50.509467Z","shell.execute_reply":"2023-03-18T12:53:50.516989Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"plane \t NOUN\n","output_type":"stream"}]}]}