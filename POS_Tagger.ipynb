{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65f9ff7",
   "metadata": {
    "papermill": {
     "duration": 0.011603,
     "end_time": "2023-03-18T12:55:43.816968",
     "exception": false,
     "start_time": "2023-03-18T12:55:43.805365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1c0e098",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:43.837808Z",
     "iopub.status.busy": "2023-03-18T12:55:43.837350Z",
     "iopub.status.idle": "2023-03-18T12:55:46.429779Z",
     "shell.execute_reply": "2023-03-18T12:55:46.428485Z"
    },
    "papermill": {
     "duration": 2.606159,
     "end_time": "2023-03-18T12:55:46.432773",
     "exception": false,
     "start_time": "2023-03-18T12:55:43.826614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import resources\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "947d324d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:46.455340Z",
     "iopub.status.busy": "2023-03-18T12:55:46.454350Z",
     "iopub.status.idle": "2023-03-18T12:55:46.465248Z",
     "shell.execute_reply": "2023-03-18T12:55:46.464042Z"
    },
    "papermill": {
     "duration": 0.02501,
     "end_time": "2023-03-18T12:55:46.467917",
     "exception": false,
     "start_time": "2023-03-18T12:55:46.442907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_conllu_file(filename):\n",
    "    index = []\n",
    "    words = []\n",
    "    pos_tags = []\n",
    "    word_sentence = []\n",
    "    pos_tags_sentences = []\n",
    "    index_sentence = []\n",
    "    words_total = []\n",
    "    pos_tags_total = []\n",
    "\n",
    "    count = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                if line.startswith(\"#\"):\n",
    "                    continue\n",
    "                fields = line.strip().split(\"\\t\")\n",
    "                if \".\" in fields[0]:\n",
    "                    continue\n",
    "                index_sentence.append(fields[0])\n",
    "                word_sentence.append(fields[1])\n",
    "                pos_tags_sentences.append(fields[3])\n",
    "                words_total.append(fields[1])\n",
    "                pos_tags_total.append(fields[3])\n",
    "            else:\n",
    "                index.append(index_sentence)\n",
    "                print\n",
    "                pos_tags.append(pos_tags_sentences)\n",
    "                words.append(word_sentence)\n",
    "                word_sentence = []\n",
    "                pos_tags_sentences = []\n",
    "                index_sentence = []\n",
    "#             print ( fields[0])\n",
    "    return index, words, pos_tags, words_total, pos_tags_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c89c2068",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:46.489684Z",
     "iopub.status.busy": "2023-03-18T12:55:46.489289Z",
     "iopub.status.idle": "2023-03-18T12:55:46.793606Z",
     "shell.execute_reply": "2023-03-18T12:55:46.792478Z"
    },
    "papermill": {
     "duration": 0.318991,
     "end_time": "2023-03-18T12:55:46.796464",
     "exception": false,
     "start_time": "2023-03-18T12:55:46.477473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "index, words, pos_tags, words_total, pos_tags_total= read_conllu_file(\"/kaggle/input/datasent-for-pos-tagging-iiith/en_atis-ud-train.conllu\")\n",
    "index_dev, words_dev, pos_tags_dev, words_total_dev, pos_tags_total_dev= read_conllu_file(\"/kaggle/input/datasent-for-pos-tagging-iiith/en_atis-ud-dev.conllu\")\n",
    "index_test, words_test, pos_tags_test, words_total_test, pos_tags_total_test= read_conllu_file(\"/kaggle/input/datasent-for-pos-tagging-iiith/en_atis-ud-test.conllu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e61d0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:46.817081Z",
     "iopub.status.busy": "2023-03-18T12:55:46.816664Z",
     "iopub.status.idle": "2023-03-18T12:55:46.822594Z",
     "shell.execute_reply": "2023-03-18T12:55:46.821397Z"
    },
    "papermill": {
     "duration": 0.021154,
     "end_time": "2023-03-18T12:55:46.827157",
     "exception": false,
     "start_time": "2023-03-18T12:55:46.806003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'would', 'like', 'the', 'cheapest', 'flight', 'from', 'pittsburgh', 'to', 'atlanta', 'leaving', 'april', 'twenty', 'fifth', 'and', 'returning', 'may', 'sixth']\n"
     ]
    }
   ],
   "source": [
    "print(words_dev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e41d7072",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:46.847896Z",
     "iopub.status.busy": "2023-03-18T12:55:46.847495Z",
     "iopub.status.idle": "2023-03-18T12:55:46.860607Z",
     "shell.execute_reply": "2023-03-18T12:55:46.859611Z"
    },
    "papermill": {
     "duration": 0.026898,
     "end_time": "2023-03-18T12:55:46.863692",
     "exception": false,
     "start_time": "2023-03-18T12:55:46.836794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total number of sentences  4274\n",
      " Total number of unique words  863\n",
      " Total number of tags  13\n"
     ]
    }
   ],
   "source": [
    "num_sentences = len(index)\n",
    "num_words = len(set(words_total))\n",
    "num_tags = len(set(pos_tags_total))\n",
    "print( \" Total number of sentences \", num_sentences)\n",
    "print( \" Total number of unique words \", num_words)\n",
    "print( \" Total number of tags \", num_tags)\n",
    "# print( set ( pos_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84185930",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:46.885457Z",
     "iopub.status.busy": "2023-03-18T12:55:46.885041Z",
     "iopub.status.idle": "2023-03-18T12:55:46.893878Z",
     "shell.execute_reply": "2023-03-18T12:55:46.892648Z"
    },
    "papermill": {
     "duration": 0.022487,
     "end_time": "2023-03-18T12:55:46.896430",
     "exception": false,
     "start_time": "2023-03-18T12:55:46.873943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total number of sentences  570\n",
      " Total number of unique words  448\n",
      " Total number of tags  13\n",
      "{'ADV', 'PROPN', 'ADP', 'NOUN', 'AUX', 'CCONJ', 'PRON', 'NUM', 'PART', 'DET', 'ADJ', 'INTJ', 'VERB'}\n"
     ]
    }
   ],
   "source": [
    "num_sentences_dev = len(index_dev)\n",
    "num_words_dev = len(set(words_total_dev))\n",
    "num_tags_dev = len(set(pos_tags_total_dev))\n",
    "print( \" Total number of sentences \", num_sentences_dev)\n",
    "print( \" Total number of unique words \", num_words_dev)\n",
    "print( \" Total number of tags \", num_tags_dev)\n",
    "print(set(pos_tags_total_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab2bc6c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:46.918511Z",
     "iopub.status.busy": "2023-03-18T12:55:46.918092Z",
     "iopub.status.idle": "2023-03-18T12:55:46.924797Z",
     "shell.execute_reply": "2023-03-18T12:55:46.923858Z"
    },
    "papermill": {
     "duration": 0.020671,
     "end_time": "2023-03-18T12:55:46.927644",
     "exception": false,
     "start_time": "2023-03-18T12:55:46.906973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total number of sentences  586\n",
      " Total number of unique words  476\n",
      " Total number of tags  13\n"
     ]
    }
   ],
   "source": [
    "num_sentences_test = len(index_test)\n",
    "num_words_test = len(set(words_total_test))\n",
    "num_tags_test = len(set(pos_tags_total_test))\n",
    "print( \" Total number of sentences \", num_sentences_test)\n",
    "print( \" Total number of unique words \", num_words_test)\n",
    "print( \" Total number of tags \", num_tags_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54722f2e",
   "metadata": {
    "papermill": {
     "duration": 0.009169,
     "end_time": "2023-03-18T12:55:46.946503",
     "exception": false,
     "start_time": "2023-03-18T12:55:46.937334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Encoding and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffa42dd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:46.967729Z",
     "iopub.status.busy": "2023-03-18T12:55:46.967255Z",
     "iopub.status.idle": "2023-03-18T12:55:47.102799Z",
     "shell.execute_reply": "2023-03-18T12:55:47.101384Z"
    },
    "papermill": {
     "duration": 0.149603,
     "end_time": "2023-03-18T12:55:47.105771",
     "exception": false,
     "start_time": "2023-03-18T12:55:46.956168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import Vocab\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73063a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:47.127438Z",
     "iopub.status.busy": "2023-03-18T12:55:47.127028Z",
     "iopub.status.idle": "2023-03-18T12:55:47.157915Z",
     "shell.execute_reply": "2023-03-18T12:55:47.156059Z"
    },
    "papermill": {
     "duration": 0.044897,
     "end_time": "2023-03-18T12:55:47.160500",
     "exception": false,
     "start_time": "2023-03-18T12:55:47.115603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 6598\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "\n",
    "unique_words = set(words_total)\n",
    "\n",
    "# print( unique_words)\n",
    "for i, word in enumerate(unique_words):\n",
    "    word_to_idx[word] = i\n",
    "    idx_to_word[i] = word\n",
    "    \n",
    "X = []\n",
    "for sent in words:\n",
    "#     print(sent)\n",
    "    encoded_sent = [word_to_idx[word] for word in sent]\n",
    "    X.append(encoded_sent)\n",
    "    \n",
    "X_test = []\n",
    "for sent in words_test:\n",
    "    encoded_sent = []\n",
    "    for word in sent:\n",
    "        if word in unique_words:\n",
    "            encoded_sent.append(word_to_idx[word])\n",
    "        else:\n",
    "            encoded_sent.append(0)\n",
    "    X_test.append(encoded_sent)\n",
    "\n",
    "count = 0 \n",
    "counta = 0 \n",
    "X_dev = []\n",
    "for sent in words_dev:\n",
    "    encoded_sent = []\n",
    "    for word in sent:\n",
    "        if word in unique_words:\n",
    "            encoded_sent.append(word_to_idx[word])\n",
    "            counta +=1\n",
    "        else:\n",
    "            encoded_sent.append(0)\n",
    "            count+=1\n",
    "    X_dev.append(encoded_sent)\n",
    "    \n",
    "# print(X_dev[4])\n",
    "print(count, counta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fb8ce6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:47.181626Z",
     "iopub.status.busy": "2023-03-18T12:55:47.181196Z",
     "iopub.status.idle": "2023-03-18T12:55:47.206066Z",
     "shell.execute_reply": "2023-03-18T12:55:47.204501Z"
    },
    "papermill": {
     "duration": 0.038623,
     "end_time": "2023-03-18T12:55:47.208778",
     "exception": false,
     "start_time": "2023-03-18T12:55:47.170155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 3, 4, 9, 10, 12, 2, 1, 2, 1, 4, 12, 7, 10, 5, 4, 12, 10]\n"
     ]
    }
   ],
   "source": [
    "word_to_idx_tag = {}\n",
    "idx_to_word_tag = {}\n",
    "\n",
    "unique_words = set(pos_tags_total)\n",
    "# print( unique_words)\n",
    "for i, word in enumerate(unique_words):\n",
    "    word_to_idx_tag[word] = i\n",
    "    idx_to_word_tag[i] = word\n",
    "    \n",
    "Y = []\n",
    "for sent in pos_tags:\n",
    "    encoded_sent = [word_to_idx_tag[word] for word in sent]\n",
    "    Y.append(encoded_sent)\n",
    "    \n",
    "Y_dev = []\n",
    "for sent in pos_tags_dev:\n",
    "    encoded_sent = [word_to_idx_tag[word] for word in sent]\n",
    "    Y_dev.append(encoded_sent)\n",
    "\n",
    "Y_test = []\n",
    "for sent in pos_tags_test:\n",
    "    encoded_sent = [word_to_idx_tag[word] for word in sent]\n",
    "    Y_test.append(encoded_sent)\n",
    "\n",
    "print(Y_dev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6159102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:47.230425Z",
     "iopub.status.busy": "2023-03-18T12:55:47.230008Z",
     "iopub.status.idle": "2023-03-18T12:55:47.235368Z",
     "shell.execute_reply": "2023-03-18T12:55:47.234187Z"
    },
    "papermill": {
     "duration": 0.018977,
     "end_time": "2023-03-18T12:55:47.237776",
     "exception": false,
     "start_time": "2023-03-18T12:55:47.218799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# word_to_idx = {}\n",
    "# idx_to_word = {}\n",
    "\n",
    "# unique_words_dev = set(pos_tags_total_dev)\n",
    "# # print( unique_words)\n",
    "# for i, word in enumerate(unique_words_dev):\n",
    "#     word_to_idx[word] = i\n",
    "#     idx_to_word[i] = word\n",
    "# Y_dev = []\n",
    "# for sent in pos_tags_dev:\n",
    "#     encoded_sent = [word_to_idx[word] for word in sent]\n",
    "#     Y_dev.append(encoded_sent)    \n",
    "\n",
    "    \n",
    "# print(Y_dev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adb8ebb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:47.259731Z",
     "iopub.status.busy": "2023-03-18T12:55:47.259334Z",
     "iopub.status.idle": "2023-03-18T12:55:47.264194Z",
     "shell.execute_reply": "2023-03-18T12:55:47.263003Z"
    },
    "papermill": {
     "duration": 0.018605,
     "end_time": "2023-03-18T12:55:47.266487",
     "exception": false,
     "start_time": "2023-03-18T12:55:47.247882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# word_to_idx = {}\n",
    "# idx_to_word = {}\n",
    "\n",
    "# unique_words_test = set(pos_tags_total_test)\n",
    "# # print( unique_words)\n",
    "# for i, word in enumerate(unique_words_test):\n",
    "#     word_to_idx[word] = i\n",
    "#     idx_to_word[i] = word\n",
    "    \n",
    "\n",
    "    \n",
    "# print(Y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22502581",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:47.288048Z",
     "iopub.status.busy": "2023-03-18T12:55:47.287562Z",
     "iopub.status.idle": "2023-03-18T12:55:47.293996Z",
     "shell.execute_reply": "2023-03-18T12:55:47.292820Z"
    },
    "papermill": {
     "duration": 0.020117,
     "end_time": "2023-03-18T12:55:47.296475",
     "exception": false,
     "start_time": "2023-03-18T12:55:47.276358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'intercontinental', 1: 'equipment', 2: 'called', 3: 'listing', 4: 'price', 5: 'greatest', 6: 'to', 7: '212', 8: 'transport', 9: 'near', 10: 'hopefully', 11: 'fridays', 12: 'seating', 13: 'no', 14: 'smallest', 15: 'afternoon', 16: 'american', 17: 'd10', 18: 'last', 19: 'trips', 20: '139', 21: 'belong', 22: 'dollars', 23: 'continental', 24: 'ontario', 25: 'newark', 26: '1940', 27: 'connection', 28: 'transportation', 29: 'times', 30: 'buy', 31: 'then', 32: 'with', 33: 'sounds', 34: 'coming', 35: 'lives', 36: '300', 37: 'any', 38: 'beginning', 39: 'kw', 40: '705', 41: 'has', 42: 'tennessee', 43: '72s', 44: 'round', 45: 'pm', 46: 'scenario', 47: 'landing', 48: 'stopping', 49: '934', 50: 'ap', 51: 'continent', 52: '1291', 53: '718', 54: 'june', 55: \"o'clock\", 56: 'fort', 57: 'inform', 58: 'fourth', 59: '1800', 60: 'trying', 61: 'hou', 62: 'baltimore', 63: 'nevada', 64: 'also', 65: 'rates', 66: 'y', 67: 'red', 68: 'connections', 69: 'columbus', 70: '12', 71: 'northwest', 72: 'least', 73: 'would', 74: 'along', 75: 'hello', 76: 'home', 77: 'evening', 78: '1158', 79: '21', 80: 'ap57', 81: '3357', 82: 'located', 83: 'january', 84: 'long', 85: '639', 86: 'latest', 87: 'somebody', 88: 'inexpensive', 89: 'having', 90: 'ninth', 91: '428', 92: 'turboprop', 93: 'live', 94: 'nighttime', 95: '932', 96: '4', 97: 'bna', 98: 'houston', 99: 'trip', 100: '7', 101: 'much', 102: 'chicago', 103: 'following', 104: 'twelfth', 105: 'designate', 106: 'some', 107: '555', 108: 'leaving', 109: 'fares', 110: 'am', 111: '150', 112: '1200', 113: 'lax', 114: 'cincinnati', 115: 'ea', 116: '1230', 117: 'preferably', 118: '757', 119: 'between', 120: 'airline', 121: 'jersey', 122: 'taxi', 123: 'anything', 124: 'charlotte', 125: 'ua', 126: 'provided', 127: '1500', 128: 'lufthansa', 129: '2100', 130: 'when', 131: 'seventeenth', 132: 'tonight', 133: 'about', 134: 'layover', 135: 'december', 136: '852', 137: '730', 138: 'traveling', 139: 'look', 140: 'have', 141: 'midnight', 142: 'can', 143: 'my', 144: 'into', 145: 'under', 146: 'begins', 147: 'let', 148: 'great', 149: 'know', 150: '1039', 151: 'sorry', 152: 'georgia', 153: 'sunday', 154: 'twentieth', 155: 'via', 156: 'highest', 157: 'atlanta', 158: 'california', 159: 'sundays', 160: 'thank', 161: 'six', 162: 'thursdays', 163: 'friday', 164: 'make', 165: 'business', 166: 'most', 167: 'return', 168: 'dca', 169: '73s', 170: 'lands', 171: 'heading', 172: 'various', 173: 'serve', 174: 'world', 175: 'numbers', 176: 'exceeding', 177: 'f28', 178: 'india', 179: 'ohio', 180: 'straight', 181: 'carries', 182: 'breakfast', 183: 'served', 184: 'off', 185: 'run', 186: 'thirteenth', 187: '1700', 188: 'discount', 189: 'francisco', 190: 'or', 191: 'logan', 192: 'reservations', 193: 'hp', 194: 'airplane', 195: 'tickets', 196: 'saturdays', 197: '1850', 198: 'fourteenth', 199: 'use', 200: 'companies', 201: '505', 202: 'makes', 203: '270', 204: '3', 205: 'international', 206: 'fifth', 207: 'another', 208: 'schedules', 209: 'pittsburgh', 210: 'but', 211: 'those', 212: 'b', 213: 'rate', 214: '1992', 215: 'thing', 216: 'a.m.', 217: 'philly', 218: 'arrangements', 219: 'jet', 220: '382', 221: 'mci', 222: 'us', 223: 'arizona', 224: 'looking', 225: 'lake', 226: '755', 227: 'across', 228: 'during', 229: 'diego', 230: \"'d\", 231: 'delta', 232: 'plane', 233: 'as', 234: 'up', 235: 'salt', 236: 'boeing', 237: '466', 238: 'scheduled', 239: 'noon', 240: 'san', 241: 'oakland', 242: 'september', 243: 'burbank', 244: 'time', 245: 'cheapest', 246: 'week', 247: 'else', 248: 'mealtime', 249: 'h', 250: 'regarding', 251: 'tomorrow', 252: 'types', 253: 'without', 254: 'west', 255: 'directly', 256: 'general', 257: 'next', 258: 'distance', 259: '217', 260: 'does', 261: 'la', 262: 'reservation', 263: '257', 264: 'stands', 265: 'priced', 266: 'sfo', 267: 'usa', 268: '530', 269: 'name', 270: '4400', 271: 'leave', 272: 'texas', 273: 'tell', 274: 'service', 275: 'number', 276: 'making', 277: 'them', 278: 'weekdays', 279: 'operation', 280: 'area', 281: 'lester', 282: 'available', 283: 'at', 284: 'love', 285: 'cvg', 286: 'kindly', 287: 'thanks', 288: '110', 289: 'dh8', 290: 'who', 291: '1993', 292: 'abbreviation', 293: 'other', 294: '130', 295: 'f', 296: 'take', 297: 'phoenix', 298: 'united', 299: 'sixteen', 300: '324', 301: 'saturday', 302: 'flying', 303: 'people', 304: 'nonstop', 305: 'st.', 306: 'takeoffs', 307: 'westchester', 308: 'car', 309: 'tacoma', 310: 'afterwards', 311: 'around', 312: '645', 313: 'book', 314: 'sixteenth', 315: 'arrives', 316: 'airplanes', 317: 'database', 318: 'of', 319: 'memphis', 320: 'their', 321: 'abbreviations', 322: 'vicinity', 323: 'ap80', 324: 'on', 325: 'there', 326: 'airport', 327: '1017', 328: 'hours', 329: 'l1011', 330: 'restrictions', 331: 'and', 332: 'route', 333: '1000', 334: 'dc10', 335: '746', 336: 'such', 337: 'serves', 338: 'first', 339: 'soon', 340: '1110', 341: 'qw', 342: '1207', 343: 'utah', 344: 'what', 345: 'third', 346: 'bay', 347: 'direct', 348: 'only', 349: 'out', 350: 'february', 351: 'carried', 352: '767', 353: 'spend', 354: 'possible', 355: 'ff', 356: 'meals', 357: 'petersburg', 358: 'kennedy', 359: \"'re\", 360: '416', 361: 'ever', 362: 'fare', 363: \"n't\", 364: 'type', 365: 'going', 366: 'flights', 367: 'will', 368: '230', 369: 'seats', 370: 'overnight', 371: 'so', 372: 'provide', 373: 'departure', 374: '82', 375: 'bh', 376: '2', 377: 'oak', 378: 'schedule', 379: 'taking', 380: 'america', 381: 'lga', 382: 'wish', 383: 'cost', 384: '1245', 385: 'too', 386: 'eye', 387: '497766', 388: 'whether', 389: 'eleventh', 390: 'earliest', 391: 'kansas', 392: 'for', 393: 'yyz', 394: 'k', 395: 'arrival', 396: 'find', 397: 'while', 398: 'same', 399: 'thirty', 400: 'midway', 401: 'los', 402: 'calling', 403: 'describe', 404: 'serving', 405: 'offered', 406: 'kind', 407: 'reaching', 408: 'supper', 409: 'late', 410: 'less', 411: 'departures', 412: 'put', 413: '345', 414: 'days', 415: 'represented', 416: 'from', 417: 'could', 418: 'express', 419: 'bur', 420: 'i', 421: 'come', 422: '0900', 423: 'dfw', 424: '733', 425: 'meaning', 426: 'aa', 427: 'minneapolis', 428: 'snack', 429: 'miami', 430: '747', 431: 'services', 432: 'final', 433: 'starting', 434: 'eighth', 435: 'being', 436: 'closest', 437: 'where', 438: '1100', 439: '723', 440: 'hi', 441: 'dc', 442: 'how', 443: '430', 444: 'goes', 445: 'eighteenth', 446: 'toward', 447: 'boston', 448: '459', 449: 'reverse', 450: 'limousines', 451: 'missouri', 452: 'before', 453: 'eastern', 454: '71', 455: '1209', 456: 'dinnertime', 457: 'guardia', 458: 'cleveland', 459: 'thrift', 460: '1220', 461: 'wn', 462: 'limo', 463: 'cars', 464: 'beach', 465: 'limousine', 466: 'july', 467: 'day', 468: 'washington', 469: 'pearson', 470: '20', 471: 'back', 472: 'do', 473: 'one', 474: '225', 475: 'qualify', 476: '420', 477: 'here', 478: 'carolina', 479: '813', 480: 'rentals', 481: 'twa', 482: 'city', 483: 'right', 484: 'more', 485: 'booking', 486: 'question', 487: 'atl', 488: 'trans', 489: 'seattle', 490: 'information', 491: 'airports', 492: 'd9s', 493: 'seventeen', 494: '720', 495: 'cities', 496: 'need', 497: 'stopover', 498: 'it', 499: 'oh', 500: 'coach', 501: 'list', 502: 'florida', 503: 'basis', 504: '305', 505: 'weekday', 506: 'second', 507: 'must', 508: 'in', 509: 'want', 510: 'way', 511: 'economy', 512: 'like', 513: '311', 514: 'combination', 515: 'earlier', 516: '1205', 517: 'stopovers', 518: 'alaska', 519: '200', 520: 'visit', 521: 'zone', 522: 'shortest', 523: 'after', 524: 'display', 525: 'including', 526: 'far', 527: 'working', 528: 'lunch', 529: 'sometime', 530: 'october', 531: 'amount', 532: 'stand', 533: 'cheap', 534: 'field', 535: 'approximately', 536: 'not', 537: 'classes', 538: 'we', 539: 'bring', 540: 'connect', 541: 'dl', 542: 'arrivals', 543: 'tuesday', 544: '329', 545: 'ac', 546: 'dulles', 547: 'today', 548: 'michigan', 549: 'got', 550: 'by', 551: 'advertises', 552: 'bound', 553: 'nonstops', 554: 'either', 555: 'wednesdays', 556: 'using', 557: '3724', 558: 'm80', 559: 'louis', 560: 'charges', 561: 'thereafter', 562: '1145', 563: 'land', 564: \"'ll\", 565: '630', 566: 'class', 567: '269', 568: 'tower', 569: '727', 570: 'worth', 571: 'later', 572: 'maximum', 573: 'nationair', 574: 'good', 575: 'train', 576: 'equal', 577: 'yn', 578: '1', 579: '823', 580: 'sort', 581: 'arrange', 582: 'ap68', 583: 'please', 584: 'level', 585: 'month', 586: 'includes', 587: 'the', 588: 'denver', 589: 'air', 590: 'capacity', 591: '8', 592: 'costs', 593: 'say', 594: 'bwi', 595: \"'m\", 596: '163', 597: '771', 598: 'again', 599: 'requesting', 600: 'montreal', 601: 'prices', 602: 'colorado', 603: '5', 604: 'april', 605: 'anywhere', 606: '320', 607: 'co', 608: 'besides', 609: 'miles', 610: 'code', 611: 'very', 612: 'travels', 613: 'names', 614: 'l10', 615: 'within', 616: 'friends', 617: '825', 618: 'north', 619: 'capacities', 620: '845', 621: '10', 622: 'longest', 623: 'arriving', 624: '1130', 625: '608', 626: 'may', 627: 'interested', 628: 'indianapolis', 629: 'should', 630: 'qo', 631: 'destination', 632: \"'s\", 633: 'four', 634: 'continuing', 635: '11', 636: 'be', 637: '400', 638: 'different', 639: '1600', 640: 'fifteen', 641: 'plan', 642: 'ten', 643: 'largest', 644: 'jfk', 645: 'total', 646: 'that', 647: 'nw', 648: 'okay', 649: 'night', 650: 'help', 651: '419', 652: \"'ve\", 653: 'listed', 654: '9', 655: 'explain', 656: 'is', 657: 'connects', 658: 'transcontinental', 659: 'me', 660: 'mondays', 661: 'an', 662: 'daily', 663: '1222', 664: 'ls', 665: 'all', 666: '1059', 667: 'ord', 668: 'york', 669: 'sure', 670: '271', 671: 'are', 672: 'close', 673: 'they', 674: 'single', 675: 'restriction', 676: 'iah', 677: 'pennsylvania', 678: 'if', 679: 'be1', 680: 'seat', 681: 'grounds', 682: 'new', 683: '296', 684: 'minimum', 685: 'symbols', 686: 'order', 687: 'planes', 688: 'now', 689: 'tenth', 690: '417', 691: '2134', 692: 'expensive', 693: 'milwaukee', 694: 'include', 695: '1055', 696: 'repeat', 697: '1994', 698: 'instead', 699: 'las', 700: 'local', 701: '315', 702: '1024', 703: 'morning', 704: 'rent', 705: '1505', 706: 'which', 707: '1020', 708: 'november', 709: 'early', 710: 'canadian', 711: 'your', 712: 'uses', 713: 'sam', 714: 'thirtieth', 715: 'snacks', 716: 'itinerary', 717: 'used', 718: 'planning', 719: 'twenty', 720: 'detroit', 721: '1030', 722: 'through', 723: 'jose', 724: 'over', 725: 'kinds', 726: 'noontime', 727: 'ground', 728: 'catch', 729: 'summer', 730: 'actually', 731: 'choices', 732: '1045', 733: 'prefer', 734: 'codes', 735: 'serviced', 736: 'angeles', 737: 'comes', 738: '124', 739: '539', 740: 'lowest', 741: 'go', 742: 'proper', 743: 'offers', 744: '737', 745: 'q', 746: '281', 747: 'rental', 748: 'see', 749: 'options', 750: 'paul', 751: 'determine', 752: 'mornings', 753: 'august', 754: 'you', 755: 'downtown', 756: 'monday', 757: 'fifteenth', 758: 'yes', 759: 'leaves', 760: 'airfare', 761: 'lastest', 762: 'quebec', 763: '201', 764: 'give', 765: 'midwest', 766: 'stop', 767: 'depart', 768: 'meal', 769: 'county', 770: 'flight', 771: 'fly', 772: 'aircraft', 773: 'fit', 774: 'vegas', 775: 's', 776: 'thursday', 777: 'dallas', 778: 'mitchell', 779: 'nights', 780: '1300', 781: 'each', 782: 'reaches', 783: 'minnesota', 784: 'two', 785: 'included', 786: '210', 787: '80', 788: 'both', 789: 'ewr', 790: 'nineteenth', 791: '734', 792: 'hold', 793: '1765', 794: 'well', 795: 'fn', 796: 'departs', 797: 'mco', 798: 'orlando', 799: 'passengers', 800: 'toronto', 801: 'qx', 802: 'than', 803: 'canada', 804: 'many', 805: 'm', 806: 'dinner', 807: '297', 808: 'tuesdays', 809: 'stapleton', 810: 'arrive', 811: 'returning', 812: 'a', 813: 'economic', 814: '1026', 815: 'seventh', 816: 'get', 817: '1991', 818: 'nashville', 819: 'offer', 820: 'define', 821: '838', 822: 'travel', 823: 'these', 824: 'originating', 825: 'philadelphia', 826: 'connecting', 827: 'indiana', 828: 'fine', 829: 'sixth', 830: 'southwest', 831: 'tampa', 832: 'wednesday', 833: 'takeoff', 834: 'mean', 835: '1115', 836: 'difference', 837: 'wants', 838: 'departing', 839: 'three', 840: '106', 841: '57', 842: 'still', 843: '343', 844: 'march', 845: 'eight', 846: 'able', 847: '100', 848: 'try', 849: 'staying', 850: 'show', 851: 'j31', 852: '405', 853: 'this', 854: '6', 855: 'flies', 856: 'landings', 857: '486', 858: 'airlines', 859: 'town', 860: 'stops', 861: 'ticket', 862: 'seven'}\n"
     ]
    }
   ],
   "source": [
    "print( idx_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc417d2",
   "metadata": {
    "papermill": {
     "duration": 0.009959,
     "end_time": "2023-03-18T12:55:47.316653",
     "exception": false,
     "start_time": "2023-03-18T12:55:47.306694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f0e9231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:47.338486Z",
     "iopub.status.busy": "2023-03-18T12:55:47.338058Z",
     "iopub.status.idle": "2023-03-18T12:55:47.345522Z",
     "shell.execute_reply": "2023-03-18T12:55:47.343775Z"
    },
    "papermill": {
     "duration": 0.022561,
     "end_time": "2023-03-18T12:55:47.349172",
     "exception": false,
     "start_time": "2023-03-18T12:55:47.326611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "length = (len(seq) for seq in X)\n",
    "print(format(max(length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e0d17c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:47.371796Z",
     "iopub.status.busy": "2023-03-18T12:55:47.371366Z",
     "iopub.status.idle": "2023-03-18T12:55:56.739466Z",
     "shell.execute_reply": "2023-03-18T12:55:56.738250Z"
    },
    "papermill": {
     "duration": 9.382367,
     "end_time": "2023-03-18T12:55:56.742287",
     "exception": false,
     "start_time": "2023-03-18T12:55:47.359920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 46\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "x_padded = pad_sequences(X,maxlen = MAX_SEQ_LENGTH, padding = 'pre', truncating = 'post' )\n",
    "y_padded = pad_sequences(Y,maxlen = MAX_SEQ_LENGTH, padding = 'pre', truncating = 'post' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ff6cd95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:56.765638Z",
     "iopub.status.busy": "2023-03-18T12:55:56.764539Z",
     "iopub.status.idle": "2023-03-18T12:55:56.773614Z",
     "shell.execute_reply": "2023-03-18T12:55:56.771813Z"
    },
    "papermill": {
     "duration": 0.0251,
     "end_time": "2023-03-18T12:55:56.777738",
     "exception": false,
     "start_time": "2023-03-18T12:55:56.752638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0 344 656 587 383 318 812  44  99 770 416 209   6 157\n",
      "  38 324 604 719 206 331 811 324 626 829]\n"
     ]
    }
   ],
   "source": [
    "print(x_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4264b8c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:56.800880Z",
     "iopub.status.busy": "2023-03-18T12:55:56.799711Z",
     "iopub.status.idle": "2023-03-18T12:55:56.843216Z",
     "shell.execute_reply": "2023-03-18T12:55:56.841626Z"
    },
    "papermill": {
     "duration": 0.058029,
     "end_time": "2023-03-18T12:55:56.846260",
     "exception": false,
     "start_time": "2023-03-18T12:55:56.788231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0, 344, 656, 587, 383, 318,\n",
      "        812,  44,  99, 770, 416, 209,   6, 157,  38, 324, 604, 719, 206, 331,\n",
      "        811, 324, 626, 829], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(torch.from_numpy(np.array(x_padded[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee4f700f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:56.869415Z",
     "iopub.status.busy": "2023-03-18T12:55:56.868243Z",
     "iopub.status.idle": "2023-03-18T12:55:56.879764Z",
     "shell.execute_reply": "2023-03-18T12:55:56.878700Z"
    },
    "papermill": {
     "duration": 0.026109,
     "end_time": "2023-03-18T12:55:56.882631",
     "exception": false,
     "start_time": "2023-03-18T12:55:56.856522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  conversion done here to convert it into a proper training data set.\n",
    "training_data = []\n",
    "for i in range(num_sentences):\n",
    "    training_data.append((X[i],Y[i]))\n",
    "    \n",
    "dev_data = []\n",
    "for i in range(num_sentences_dev):\n",
    "    dev_data.append((X_dev[i],Y_dev[i]))\n",
    "    \n",
    "test_data = []\n",
    "for i in range(num_sentences_test):\n",
    "    test_data.append((X_test[i],Y_test[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "332a68e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:56.905431Z",
     "iopub.status.busy": "2023-03-18T12:55:56.905012Z",
     "iopub.status.idle": "2023-03-18T12:55:56.910909Z",
     "shell.execute_reply": "2023-03-18T12:55:56.909683Z"
    },
    "papermill": {
     "duration": 0.020833,
     "end_time": "2023-03-18T12:55:56.913855",
     "exception": false,
     "start_time": "2023-03-18T12:55:56.893022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n",
      "([344, 671, 587, 500, 366, 119, 777, 331, 62, 108, 753, 689, 331, 811, 753, 0], [6, 3, 9, 12, 12, 2, 1, 5, 1, 4, 12, 10, 5, 4, 12, 7])\n"
     ]
    }
   ],
   "source": [
    "print(\"d\")\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2abe1e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:56.936537Z",
     "iopub.status.busy": "2023-03-18T12:55:56.936082Z",
     "iopub.status.idle": "2023-03-18T12:55:56.941889Z",
     "shell.execute_reply": "2023-03-18T12:55:56.940597Z"
    },
    "papermill": {
     "duration": 0.020272,
     "end_time": "2023-03-18T12:55:56.944573",
     "exception": false,
     "start_time": "2023-03-18T12:55:56.924301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # training sentences and their corresponding word-tags\n",
    "# training_data = [\n",
    "#     (\"The cat ate the cheese\".lower().split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "#     (\"She read that book\".lower().split(), [\"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "#     (\"The dog loves art\".lower().split(), [\"DET\", \"NN\", \"V\", \"NN\"]),\n",
    "#     (\"The elephant answers the phone\".lower().split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"])\n",
    "# ]\n",
    "\n",
    "# # create a dictionary that maps words to indices\n",
    "# word2idx = {}\n",
    "\n",
    "# for sent, tags in training_data:\n",
    "    \n",
    "#     for word in sent:\n",
    "        \n",
    "#         if word not in word2idx:\n",
    "            \n",
    "#             word2idx[word] = len(word2idx)\n",
    "\n",
    "# # create a dictionary that maps tags to indices\n",
    "# tag2idx = {\"DET\": 0, \"NN\": 1, \"V\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17fe1978",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:56.967491Z",
     "iopub.status.busy": "2023-03-18T12:55:56.966811Z",
     "iopub.status.idle": "2023-03-18T12:55:56.972156Z",
     "shell.execute_reply": "2023-03-18T12:55:56.970888Z"
    },
    "papermill": {
     "duration": 0.019666,
     "end_time": "2023-03-18T12:55:56.974673",
     "exception": false,
     "start_time": "2023-03-18T12:55:56.955007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # print out the created dictionary\n",
    "# print(word2idx)\n",
    "# print(tag2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9308bbe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:56.997584Z",
     "iopub.status.busy": "2023-03-18T12:55:56.997192Z",
     "iopub.status.idle": "2023-03-18T12:55:57.002306Z",
     "shell.execute_reply": "2023-03-18T12:55:57.001078Z"
    },
    "papermill": {
     "duration": 0.019489,
     "end_time": "2023-03-18T12:55:57.004944",
     "exception": false,
     "start_time": "2023-03-18T12:55:56.985455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # a helper function for converting a sequence of words to a Tensor of numerical values\n",
    "# # will be used later in training\n",
    "# def prepare_sequence(seq, to_idx):\n",
    "#     '''This function takes in a sequence of words and returns a \n",
    "#     corresponding Tensor of numerical values (indices for each word).'''\n",
    "    \n",
    "#     idxs = [to_idx[w] for w in seq]\n",
    "#     idxs = np.array(idxs)\n",
    "    \n",
    "#     return torch.from_numpy(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "669b190a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:57.028063Z",
     "iopub.status.busy": "2023-03-18T12:55:57.027624Z",
     "iopub.status.idle": "2023-03-18T12:55:57.032618Z",
     "shell.execute_reply": "2023-03-18T12:55:57.031422Z"
    },
    "papermill": {
     "duration": 0.019507,
     "end_time": "2023-03-18T12:55:57.035066",
     "exception": false,
     "start_time": "2023-03-18T12:55:57.015559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # check out what prepare_sequence does for one of our training sentences:\n",
    "# example_input = prepare_sequence(\"The dog answers the phone\".lower().split(), word2idx)\n",
    "\n",
    "# print(example_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c63033d",
   "metadata": {
    "papermill": {
     "duration": 0.010086,
     "end_time": "2023-03-18T12:55:57.055335",
     "exception": false,
     "start_time": "2023-03-18T12:55:57.045249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "762f05de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:57.077496Z",
     "iopub.status.busy": "2023-03-18T12:55:57.077086Z",
     "iopub.status.idle": "2023-03-18T12:55:57.089320Z",
     "shell.execute_reply": "2023-03-18T12:55:57.087864Z"
    },
    "papermill": {
     "duration": 0.026442,
     "end_time": "2023-03-18T12:55:57.091910",
     "exception": false,
     "start_time": "2023-03-18T12:55:57.065468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        ''' Initialize the layers of this model.'''\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embedding layer that turns words into a vector of a specified size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # the LSTM takes embedded word vectors (of a specified size) as inputs \n",
    "        # and outputs hidden states of size hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # the linear layer that maps the hidden state output dimension \n",
    "        # to the number of tags we want as output, tagset_size (in this case this is 3 tags)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "        # initialize the hidden state (see code below)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        ''' At the start of training, we need to initialize a hidden state;\n",
    "           there will be none because the hidden state is formed based on perviously seen data.\n",
    "           So, this function defines a hidden state with all zeroes and of a specified size.'''\n",
    "        # The axes dimensions are (n_layers, batch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        ''' Define the feedforward behavior of the model.'''\n",
    "        # create embedded word vectors for each word in a sentence\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        \n",
    "        # get the output and hidden state by passing the lstm over our word embeddings\n",
    "        # the lstm takes in our embeddings and hiddent state\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        \n",
    "        # get the scores for the most likely tag for a word\n",
    "        tag_outputs = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_outputs, dim=1)\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a517d13f",
   "metadata": {
    "papermill": {
     "duration": 0.01018,
     "end_time": "2023-03-18T12:55:57.112596",
     "exception": false,
     "start_time": "2023-03-18T12:55:57.102416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define how the model trains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b0e2473",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:57.135312Z",
     "iopub.status.busy": "2023-03-18T12:55:57.134633Z",
     "iopub.status.idle": "2023-03-18T12:55:57.183380Z",
     "shell.execute_reply": "2023-03-18T12:55:57.182296Z"
    },
    "papermill": {
     "duration": 0.063512,
     "end_time": "2023-03-18T12:55:57.186230",
     "exception": false,
     "start_time": "2023-03-18T12:55:57.122718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the embedding dimension defines the size of our word vectors\n",
    "# for our simple vocabulary and training set, we will keep these small\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 300\n",
    "\n",
    "# instantiate our model\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(index), num_tags)\n",
    "\n",
    "# define our loss and optimizer\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6579b4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:57.208726Z",
     "iopub.status.busy": "2023-03-18T12:55:57.208284Z",
     "iopub.status.idle": "2023-03-18T12:55:57.213618Z",
     "shell.execute_reply": "2023-03-18T12:55:57.212404Z"
    },
    "papermill": {
     "duration": 0.019711,
     "end_time": "2023-03-18T12:55:57.216117",
     "exception": false,
     "start_time": "2023-03-18T12:55:57.196406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_sentence = \"The cheese loves the elephant\".lower().split()\n",
    "\n",
    "# # see what the scores are before training\n",
    "# # element [i,j] of the output is the *score* for tag j for word i.\n",
    "# # to check the initial accuracy of our model, we don't need to train, so we use model.eval()\n",
    "# inputs = prepare_sequence(test_sentence, word2idx)\n",
    "# inputs = inputs\n",
    "# print(inputs)\n",
    "# tag_scores = model(inputs)\n",
    "# print(tag_scores)\n",
    "\n",
    "# # tag_scores outputs a vector of tag scores for each word in an inpit sentence\n",
    "# # to get the most likely tag index, we grab the index with the maximum score!\n",
    "# # recall that these numbers correspond to tag2idx = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "# _, predicted_tags = torch.max(tag_scores, 1)\n",
    "# print('\\n')\n",
    "# print('Predicted tags: \\n',predicted_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b8591",
   "metadata": {
    "papermill": {
     "duration": 0.010284,
     "end_time": "2023-03-18T12:55:57.236568",
     "exception": false,
     "start_time": "2023-03-18T12:55:57.226284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bcc382e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:57.258850Z",
     "iopub.status.busy": "2023-03-18T12:55:57.258407Z",
     "iopub.status.idle": "2023-03-18T12:55:58.029397Z",
     "shell.execute_reply": "2023-03-18T12:55:58.028052Z"
    },
    "papermill": {
     "duration": 0.785558,
     "end_time": "2023-03-18T12:55:58.032242",
     "exception": false,
     "start_time": "2023-03-18T12:55:57.246684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7471089a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:58.054451Z",
     "iopub.status.busy": "2023-03-18T12:55:58.054060Z",
     "iopub.status.idle": "2023-03-18T12:55:58.060246Z",
     "shell.execute_reply": "2023-03-18T12:55:58.058994Z"
    },
    "papermill": {
     "duration": 0.020829,
     "end_time": "2023-03-18T12:55:58.063279",
     "exception": false,
     "start_time": "2023-03-18T12:55:58.042450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADV', 'PROPN', 'ADP', 'AUX', 'VERB', 'CCONJ', 'PRON', 'NUM', 'PART', 'DET', 'ADJ', 'INTJ', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "    label = []\n",
    "    for i in range(len(idx_to_word_tag)):\n",
    "        label.append(idx_to_word_tag[i])\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab03f00d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:55:58.085392Z",
     "iopub.status.busy": "2023-03-18T12:55:58.084994Z",
     "iopub.status.idle": "2023-03-18T13:02:59.741897Z",
     "shell.execute_reply": "2023-03-18T13:02:59.740434Z"
    },
    "papermill": {
     "duration": 421.681272,
     "end_time": "2023-03-18T13:02:59.754777",
     "exception": false,
     "start_time": "2023-03-18T12:55:58.073505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.75      0.83        76\n",
      "           1       0.98      1.00      0.99      1567\n",
      "           2       1.00      1.00      1.00      1434\n",
      "           3       0.95      0.97      0.96       256\n",
      "           4       0.99      0.96      0.97       629\n",
      "           5       1.00      1.00      1.00       109\n",
      "           6       0.86      0.98      0.91       392\n",
      "           7       0.92      0.94      0.93       127\n",
      "           8       0.98      0.96      0.97        56\n",
      "           9       0.98      0.87      0.92       512\n",
      "          10       0.94      0.96      0.95       220\n",
      "          11       1.00      1.00      1.00        36\n",
      "          12       0.99      0.99      0.99      1166\n",
      "\n",
      "    accuracy                           0.97      6580\n",
      "   macro avg       0.96      0.95      0.96      6580\n",
      "weighted avg       0.98      0.97      0.97      6580\n",
      "\n",
      "Epoch: 1, loss: 0.16198, Train_Acc : 0.94660, dev_acc : 0.97074, test_acc : 0.97477\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.75      0.83        76\n",
      "           1       0.97      1.00      0.99      1567\n",
      "           2       1.00      1.00      1.00      1434\n",
      "           3       0.96      0.96      0.96       256\n",
      "           4       0.98      0.97      0.97       629\n",
      "           5       1.00      1.00      1.00       109\n",
      "           6       0.97      0.74      0.84       392\n",
      "           7       0.98      0.85      0.91       127\n",
      "           8       0.96      0.98      0.97        56\n",
      "           9       0.83      0.98      0.90       512\n",
      "          10       0.94      0.95      0.94       220\n",
      "          11       1.00      1.00      1.00        36\n",
      "          12       0.99      0.99      0.99      1166\n",
      "\n",
      "    accuracy                           0.97      6580\n",
      "   macro avg       0.96      0.94      0.95      6580\n",
      "weighted avg       0.97      0.97      0.97      6580\n",
      "\n",
      "Epoch: 2, loss: 0.06327, Train_Acc : 0.97402, dev_acc : 0.96757, test_acc : 0.96778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.80      0.87        76\n",
      "           1       0.97      1.00      0.99      1567\n",
      "           2       1.00      1.00      1.00      1434\n",
      "           3       0.98      0.94      0.96       256\n",
      "           4       0.97      0.97      0.97       629\n",
      "           5       1.00      1.00      1.00       109\n",
      "           6       0.88      0.94      0.91       392\n",
      "           7       0.97      0.85      0.91       127\n",
      "           8       0.98      0.98      0.98        56\n",
      "           9       0.95      0.89      0.92       512\n",
      "          10       0.93      0.96      0.95       220\n",
      "          11       1.00      1.00      1.00        36\n",
      "          12       0.99      0.99      0.99      1166\n",
      "\n",
      "    accuracy                           0.97      6580\n",
      "   macro avg       0.97      0.95      0.96      6580\n",
      "weighted avg       0.97      0.97      0.97      6580\n",
      "\n",
      "Epoch: 3, loss: 0.05471, Train_Acc : 0.97577, dev_acc : 0.97104, test_acc : 0.97356\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.74      0.82        76\n",
      "           1       0.97      1.00      0.99      1567\n",
      "           2       0.98      1.00      0.99      1434\n",
      "           3       0.94      0.99      0.96       256\n",
      "           4       0.99      0.93      0.96       629\n",
      "           5       1.00      1.00      1.00       109\n",
      "           6       1.00      0.73      0.85       392\n",
      "           7       0.99      0.85      0.92       127\n",
      "           8       0.98      0.89      0.93        56\n",
      "           9       0.83      1.00      0.90       512\n",
      "          10       0.92      0.96      0.94       220\n",
      "          11       1.00      1.00      1.00        36\n",
      "          12       1.00      0.99      0.99      1166\n",
      "\n",
      "    accuracy                           0.97      6580\n",
      "   macro avg       0.96      0.93      0.94      6580\n",
      "weighted avg       0.97      0.97      0.97      6580\n",
      "\n",
      "Epoch: 4, loss: 0.05354, Train_Acc : 0.97715, dev_acc : 0.96259, test_acc : 0.96626\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.78      0.85        76\n",
      "           1       0.98      1.00      0.99      1567\n",
      "           2       1.00      1.00      1.00      1434\n",
      "           3       0.97      0.93      0.95       256\n",
      "           4       0.97      0.96      0.97       629\n",
      "           5       1.00      1.00      1.00       109\n",
      "           6       1.00      0.73      0.85       392\n",
      "           7       0.97      0.85      0.91       127\n",
      "           8       0.98      0.98      0.98        56\n",
      "           9       0.83      1.00      0.90       512\n",
      "          10       0.93      0.96      0.94       220\n",
      "          11       1.00      1.00      1.00        36\n",
      "          12       0.99      0.99      0.99      1166\n",
      "\n",
      "    accuracy                           0.97      6580\n",
      "   macro avg       0.97      0.94      0.95      6580\n",
      "weighted avg       0.97      0.97      0.97      6580\n",
      "\n",
      "Epoch: 5, loss: 0.05260, Train_Acc : 0.97737, dev_acc : 0.96546, test_acc : 0.96733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.74      0.83        76\n",
      "           1       0.97      1.00      0.99      1567\n",
      "           2       1.00      1.00      1.00      1434\n",
      "           3       0.94      0.98      0.96       256\n",
      "           4       0.99      0.96      0.97       629\n",
      "           5       1.00      1.00      1.00       109\n",
      "           6       0.92      0.84      0.88       392\n",
      "           7       0.97      0.84      0.90       127\n",
      "           8       0.98      0.93      0.95        56\n",
      "           9       0.88      0.94      0.91       512\n",
      "          10       0.92      0.96      0.94       220\n",
      "          11       1.00      1.00      1.00        36\n",
      "          12       0.99      0.99      0.99      1166\n",
      "\n",
      "    accuracy                           0.97      6580\n",
      "   macro avg       0.96      0.94      0.95      6580\n",
      "weighted avg       0.97      0.97      0.97      6580\n",
      "\n",
      "Epoch: 6, loss: 0.05052, Train_Acc : 0.97854, dev_acc : 0.97104, test_acc : 0.96960\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.78      0.80        76\n",
      "           1       0.98      1.00      0.99      1567\n",
      "           2       0.99      0.99      0.99      1434\n",
      "           3       0.96      0.94      0.95       256\n",
      "           4       0.97      0.97      0.97       629\n",
      "           5       1.00      1.00      1.00       109\n",
      "           6       0.86      0.97      0.91       392\n",
      "           7       0.98      0.84      0.91       127\n",
      "           8       0.98      0.98      0.98        56\n",
      "           9       0.97      0.87      0.92       512\n",
      "          10       0.93      0.92      0.92       220\n",
      "          11       1.00      1.00      1.00        36\n",
      "          12       0.99      0.99      0.99      1166\n",
      "\n",
      "    accuracy                           0.97      6580\n",
      "   macro avg       0.96      0.94      0.95      6580\n",
      "weighted avg       0.97      0.97      0.97      6580\n",
      "\n",
      "Epoch: 7, loss: 0.04900, Train_Acc : 0.97854, dev_acc : 0.97240, test_acc : 0.97052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.79      0.86        76\n",
      "           1       0.97      1.00      0.99      1567\n",
      "           2       1.00      1.00      1.00      1434\n",
      "           3       0.94      0.98      0.96       256\n",
      "           4       0.99      0.96      0.97       629\n",
      "           5       1.00      0.99      1.00       109\n",
      "           6       0.87      0.97      0.92       392\n",
      "           7       0.98      0.84      0.91       127\n",
      "           8       0.98      0.96      0.97        56\n",
      "           9       0.97      0.89      0.93       512\n",
      "          10       0.93      0.96      0.95       220\n",
      "          11       1.00      1.00      1.00        36\n",
      "          12       0.99      0.99      0.99      1166\n",
      "\n",
      "    accuracy                           0.97      6580\n",
      "   macro avg       0.97      0.95      0.96      6580\n",
      "weighted avg       0.98      0.97      0.97      6580\n",
      "\n",
      "Epoch: 8, loss: 0.05216, Train_Acc : 0.97762, dev_acc : 0.97164, test_acc : 0.97477\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.72      0.82        76\n",
      "           1       0.97      1.00      0.99      1567\n",
      "           2       1.00      1.00      1.00      1434\n",
      "           3       0.96      0.95      0.96       256\n",
      "           4       0.98      0.95      0.96       629\n",
      "           5       1.00      1.00      1.00       109\n",
      "           6       0.84      0.99      0.91       392\n",
      "           7       0.96      0.84      0.90       127\n",
      "           8       0.98      0.98      0.98        56\n",
      "           9       0.99      0.86      0.92       512\n",
      "          10       0.92      0.95      0.93       220\n",
      "          11       1.00      1.00      1.00        36\n",
      "          12       0.99      0.99      0.99      1166\n",
      "\n",
      "    accuracy                           0.97      6580\n",
      "   macro avg       0.96      0.94      0.95      6580\n",
      "weighted avg       0.97      0.97      0.97      6580\n",
      "\n",
      "Epoch: 9, loss: 0.05103, Train_Acc : 0.97772, dev_acc : 0.97044, test_acc : 0.97112\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86        76\n",
      "           1       0.97      1.00      0.99      1567\n",
      "           2       1.00      1.00      1.00      1434\n",
      "           3       0.95      0.96      0.95       256\n",
      "           4       0.98      0.96      0.97       629\n",
      "           5       1.00      1.00      1.00       109\n",
      "           6       0.85      0.98      0.91       392\n",
      "           7       0.98      0.84      0.91       127\n",
      "           8       0.96      0.98      0.97        56\n",
      "           9       0.98      0.87      0.92       512\n",
      "          10       0.96      0.95      0.95       220\n",
      "          11       1.00      1.00      1.00        36\n",
      "          12       1.00      0.99      0.99      1166\n",
      "\n",
      "    accuracy                           0.97      6580\n",
      "   macro avg       0.96      0.95      0.96      6580\n",
      "weighted avg       0.98      0.97      0.97      6580\n",
      "\n",
      "Epoch: 10, loss: 0.05322, Train_Acc : 0.97743, dev_acc : 0.97240, test_acc : 0.97432\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 20\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    epoch_loss = torch.tensor(0, dtype=torch.float) \n",
    "    total_length_accuracy = 0\n",
    "    correct_length_accuracy = 0\n",
    "    # shuffle the training data\n",
    "    random.shuffle(training_data)\n",
    "    batches = []\n",
    "    # split the training data into batches\n",
    "    for batch_start in range(0, len(training_data), batch_size):\n",
    "        batch_end = batch_start + batch_size\n",
    "        batch_sentences = [data[0] for data in training_data[batch_start:batch_end]]\n",
    "        batch_tags = [data[1] for data in training_data[batch_start:batch_end]]\n",
    "        \n",
    "        batches.append(training_data[batch_start:batch_end])\n",
    "#     print(batches)\n",
    "    for batch in batches:\n",
    "        total_loss = torch.tensor(0, dtype=torch.float) \n",
    "        \n",
    "        # initialize total_loss as a PyTorch tensor object        total_length_accuracy  = 0 \n",
    "#         correct_length_accuracy  = 0 \n",
    "        # zero the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # zero the hidden state of the LSTM, this detaches it from its history\n",
    "        model.hidden = model.init_hidden()\n",
    " \n",
    "        # prepare the inputs for processing by out network, \n",
    "        # turn all sentences and targets into Tensors of numerical indices\n",
    "        for sentence, tags in batch:\n",
    "#             print(sentence)\n",
    "            targets = torch.from_numpy(np.array(tags))\n",
    "#             print(targets[1])\n",
    "            # forward pass to get tag scores\n",
    "            tag_scores = model(torch.from_numpy(np.array(sentence)))\n",
    "#         print(tag_scores)\n",
    "            # compute the loss, and gradients \n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            epoch_loss += loss\n",
    "            total_loss += loss\n",
    "            total_length_accuracy+=len(tag_scores)\n",
    "#             tag_scores = tag_scores.item()\n",
    "#             targets = targets.item()\n",
    "            _, predicted_tags = torch.max(tag_scores, 1)\n",
    "            for i in range(len(tag_scores)):\n",
    "#                 print(predicted_tags[i], targets[i])\n",
    "\n",
    "                if predicted_tags[i] == targets[i]:\n",
    "                    correct_length_accuracy+=1\n",
    "                    \n",
    "#         print(\"necttttt\")\n",
    "#         print(correct_length_accuracy, total_length_accuracy)\n",
    "        # compute the loss, and gradients \n",
    "        total_loss.backward()\n",
    "        \n",
    "        # update the model parameters with optimizer.step()\n",
    "        optimizer.step()\n",
    "        \n",
    "#   now we have to check accuracy \n",
    "#   1. accuracy on training data\n",
    "    accuracy_train = correct_length_accuracy/total_length_accuracy\n",
    "    \n",
    "#   2. accuracy on dev data set and test\n",
    "    total_length_dev = 0 \n",
    "    correct_length_dev = 0\n",
    "    total_length_test = 0\n",
    "    correct_length_test = 0\n",
    "        \n",
    "    for sentence, tags in dev_data:\n",
    "        targets = torch.from_numpy(np.array(tags))\n",
    "        tag_scores = model(torch.from_numpy(np.array(sentence)))\n",
    "        _, predicted_tags = torch.max(tag_scores, 1)\n",
    "        for i in range(len(tag_scores)):\n",
    "            if predicted_tags[i] == targets[i]:\n",
    "                correct_length_dev += 1\n",
    "        total_length_dev += len(tag_scores)\n",
    "    tags_orig = []\n",
    "    tags_pred = []\n",
    "    for sentence, tags in test_data:\n",
    "        targets = torch.from_numpy(np.array(tags))\n",
    "        for tag in tags:\n",
    "            tags_orig.append(tag)\n",
    "        tag_scores = model(torch.from_numpy(np.array(sentence)))\n",
    "        _, predicted_tags = torch.max(tag_scores, 1)\n",
    "        pred_tagu = predicted_tags.tolist()\n",
    "        for tag in pred_tagu:\n",
    "            tags_pred.append(tag)\n",
    "        for i in range(len(tag_scores)):\n",
    "            if predicted_tags[i] == targets[i]:\n",
    "                correct_length_test += 1\n",
    "        total_length_test += len(tag_scores)\n",
    "            \n",
    "        accuracy_dev = correct_length_dev/total_length_dev\n",
    "        accuracy_test = correct_length_test/total_length_test\n",
    "#     print(tags_orig, tags_pred)\n",
    "#     label = []\n",
    "#     for i in range(len(idx_to_word_tag)):\n",
    "#         label.append()\n",
    "    print(classification_report(tags_orig, tags_pred, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]))\n",
    "    # print out avg loss per epoch\n",
    "    print(\"Epoch: %d, loss: %1.5f, Train_Acc : %1.5f, dev_acc : %1.5f, test_acc : %1.5f\" % (epoch+1, epoch_loss.item()/len(training_data),accuracy_train, accuracy_dev, accuracy_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aff1d0c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T13:02:59.779719Z",
     "iopub.status.busy": "2023-03-18T13:02:59.779249Z",
     "iopub.status.idle": "2023-03-18T13:02:59.797923Z",
     "shell.execute_reply": "2023-03-18T13:02:59.796615Z"
    },
    "papermill": {
     "duration": 0.034432,
     "end_time": "2023-03-18T13:02:59.800697",
     "exception": false,
     "start_time": "2023-03-18T13:02:59.766265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34f7c409",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T13:02:59.826873Z",
     "iopub.status.busy": "2023-03-18T13:02:59.826423Z",
     "iopub.status.idle": "2023-03-18T13:02:59.833207Z",
     "shell.execute_reply": "2023-03-18T13:02:59.831893Z"
    },
    "papermill": {
     "duration": 0.022946,
     "end_time": "2023-03-18T13:02:59.835708",
     "exception": false,
     "start_time": "2023-03-18T13:02:59.812762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # normally these epochs take a lot longer \n",
    "# # but with our toy data (only 3 sentences), we can do many epochs in a short time\n",
    "# n_epochs = 10\n",
    "\n",
    "# # if torch.cuda.is_available():\n",
    "# #     device = torch.device(\"cuda\")\n",
    "# # else:\n",
    "# #     device = torch.device(\"cpu\")\n",
    "\n",
    "# # model.to(device)\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "    \n",
    "#     epoch_loss = 0.0\n",
    "    \n",
    "#     # get all sentences and corresponding tags in the training data\n",
    "#     for sentence, tags in training_data:\n",
    "        \n",
    "#         # zero the gradients\n",
    "#         model.zero_grad()        \n",
    "\n",
    "#         # zero the hidden state of the LSTM, this detaches it from its history\n",
    "#         model.hidden = model.init_hidden()\n",
    "\n",
    "#         # prepare the inputs for processing by out network, \n",
    "#         # turn all sentences and targets into Tensors of numerical indices\n",
    "# #         sentence_in = prepare_sequence(sentence, word2idx)\n",
    "#         targets = torch.from_numpy(np.array(tags))\n",
    "\n",
    "#         # forward pass to get tag scores\n",
    "#         tag_scores = model(torch.from_numpy(np.array(sentence)))\n",
    "# #         print(tag_scores)\n",
    "#         # compute the loss, and gradients \n",
    "#         loss = loss_function(tag_scores, targets)\n",
    "#         epoch_loss += loss.item()\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # update the model parameters with optimizer.step()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#     # print out avg loss per 20 epochs\n",
    "# #     if(epoch%1 == 1):\n",
    "#     print(\"Epoch: %d, loss: %1.5f\" % (epoch+1, epoch_loss/len(training_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c760a388",
   "metadata": {
    "papermill": {
     "duration": 0.011263,
     "end_time": "2023-03-18T13:02:59.858053",
     "exception": false,
     "start_time": "2023-03-18T13:02:59.846790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac5a316a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T13:02:59.882160Z",
     "iopub.status.busy": "2023-03-18T13:02:59.881710Z",
     "iopub.status.idle": "2023-03-18T13:02:59.895177Z",
     "shell.execute_reply": "2023-03-18T13:02:59.893563Z"
    },
    "papermill": {
     "duration": 0.028296,
     "end_time": "2023-03-18T13:02:59.897585",
     "exception": false,
     "start_time": "2023-03-18T13:02:59.869289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_sentence = \"plane\".lower().split()\n",
    "\n",
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "\n",
    "unique_words = set(words_total)\n",
    "\n",
    "for i, word in enumerate(unique_words):\n",
    "    word_to_idx[word] = i\n",
    "    idx_to_word[i] = word\n",
    "    \n",
    "encoded_sent = []\n",
    "for word in test_sentence:\n",
    "    encoded_sent.append(word_to_idx[word])\n",
    "    \n",
    "# print(encoded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5931944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T13:02:59.922748Z",
     "iopub.status.busy": "2023-03-18T13:02:59.922348Z",
     "iopub.status.idle": "2023-03-18T13:02:59.935233Z",
     "shell.execute_reply": "2023-03-18T13:02:59.932667Z"
    },
    "papermill": {
     "duration": 0.030273,
     "end_time": "2023-03-18T13:02:59.939025",
     "exception": false,
     "start_time": "2023-03-18T13:02:59.908752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plane \t NOUN\n"
     ]
    }
   ],
   "source": [
    "# see what the scores are after training\n",
    "# inputs = prepare_sequence(test_sentence, word2idx)\n",
    "# inputs = inputs\n",
    "tag_scores = model(torch.from_numpy(np.array(encoded_sent)))\n",
    "# print(tag_scores)\n",
    "\n",
    "# print the most likely tag index, by grabbing the index with the maximum score!\n",
    "# recall that these numbers correspond to tag2idx = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "_, predicted_tagss = torch.max(tag_scores, 1)\n",
    "ans = []\n",
    "tags = predicted_tagss.tolist()\n",
    "for idx in tags:\n",
    "    encoded_sent = idx_to_word_tag[idx]\n",
    "    ans.append(encoded_sent)\n",
    "    \n",
    "# print(idx_to_word_tag)\n",
    "# print(ans)\n",
    "# print('\\n')\n",
    "# print('Predicted tags: \\n',predicted_tagss)\n",
    "for i in range(len(ans)):\n",
    "    print(test_sentence[i], \"\\t\", ans[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 450.050448,
   "end_time": "2023-03-18T13:03:03.449398",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-18T12:55:33.398950",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
